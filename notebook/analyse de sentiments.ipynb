{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyOkNCXOcpf1KSUjb6sEpRvv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7120f27c0eb54812b36a3fa60a253df6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bfbe14ad18484e8286a212f1d92473ac","IPY_MODEL_dcbc8641589e4eb6bd33ef9bdbd26266","IPY_MODEL_7b07b6a83a0b4ea497c425ae9e591e91"],"layout":"IPY_MODEL_cae813044bd248879dbec98bd1aea171"}},"bfbe14ad18484e8286a212f1d92473ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b3efc077310b4f38b4dc42c3345a3ed5","placeholder":"​","style":"IPY_MODEL_a2f9c186af6e405293fcd79c07f1f572","value":"tokenizer_config.json: 100%"}},"dcbc8641589e4eb6bd33ef9bdbd26266":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_38e48e83164146c2aa0e3dfa8da870d8","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8effdeefab004d5eb3f35ba4eb657e10","value":48}},"7b07b6a83a0b4ea497c425ae9e591e91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd92196a116f48a1acc05b17d9f83021","placeholder":"​","style":"IPY_MODEL_050663d5ed8e4318ac7f063e6d495c42","value":" 48.0/48.0 [00:00&lt;00:00, 4.10kB/s]"}},"cae813044bd248879dbec98bd1aea171":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3efc077310b4f38b4dc42c3345a3ed5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2f9c186af6e405293fcd79c07f1f572":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"38e48e83164146c2aa0e3dfa8da870d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8effdeefab004d5eb3f35ba4eb657e10":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd92196a116f48a1acc05b17d9f83021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"050663d5ed8e4318ac7f063e6d495c42":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60ef8d7b5dc5409db0e9a85d9695bbab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c04e0efcdd8b425bbb614954991cdef9","IPY_MODEL_df74402a94574e0793836112032fd99c","IPY_MODEL_835cfc25db4f43a79db754394768bf40"],"layout":"IPY_MODEL_af6a8d68e68640c6bfe416cb45afae6c"}},"c04e0efcdd8b425bbb614954991cdef9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c4b865c26024574adaf8959f554678f","placeholder":"​","style":"IPY_MODEL_51e9ae2eeafe4ef6bc53099bfcab7cb0","value":"vocab.txt: 100%"}},"df74402a94574e0793836112032fd99c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20220f90a63c4f18915540f2783e1947","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9198722b40f44dcbd4523cbce2d8e72","value":231508}},"835cfc25db4f43a79db754394768bf40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_680a7a3ff165439a9730c88188fe5a18","placeholder":"​","style":"IPY_MODEL_09cc98fa496741758b774d31f1ff775d","value":" 232k/232k [00:00&lt;00:00, 4.89MB/s]"}},"af6a8d68e68640c6bfe416cb45afae6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c4b865c26024574adaf8959f554678f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51e9ae2eeafe4ef6bc53099bfcab7cb0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20220f90a63c4f18915540f2783e1947":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9198722b40f44dcbd4523cbce2d8e72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"680a7a3ff165439a9730c88188fe5a18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09cc98fa496741758b774d31f1ff775d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f334947127d44c8b3ee44918d2ca933":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_01e3aa6e07ef4d0f9b46ce1931320def","IPY_MODEL_e78dedec9c814ab293b804c1e81c4a16","IPY_MODEL_ad810c946d234b0bb30c9bdff208fb44"],"layout":"IPY_MODEL_8a9bfddef3434a6c8b28201eb5326063"}},"01e3aa6e07ef4d0f9b46ce1931320def":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7dc2e691ddd433d9f37cb7cb356bf8e","placeholder":"​","style":"IPY_MODEL_ad573b1732cb440ba3d077af39af6334","value":"tokenizer.json: 100%"}},"e78dedec9c814ab293b804c1e81c4a16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b06417e3cb0e429d8355142612dfaf12","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d19240c37a241d6a29f27b3f1d51226","value":466062}},"ad810c946d234b0bb30c9bdff208fb44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca5d6bc94f094f45bfa3c823a5d66b34","placeholder":"​","style":"IPY_MODEL_fa4f98b5d615432bb8577fa98eb4b9c6","value":" 466k/466k [00:00&lt;00:00, 15.4MB/s]"}},"8a9bfddef3434a6c8b28201eb5326063":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7dc2e691ddd433d9f37cb7cb356bf8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad573b1732cb440ba3d077af39af6334":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b06417e3cb0e429d8355142612dfaf12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d19240c37a241d6a29f27b3f1d51226":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca5d6bc94f094f45bfa3c823a5d66b34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa4f98b5d615432bb8577fa98eb4b9c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c654b00ac601409985492c64d4d4b839":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2519b66dd98f43daa8749c77fd6dc06e","IPY_MODEL_a16595e2883a445589a1410a659865af","IPY_MODEL_aee64cd2a2ca4cb0887f02df370a4f08"],"layout":"IPY_MODEL_3e5437c01143454aa6d751868f32324b"}},"2519b66dd98f43daa8749c77fd6dc06e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1429522ede914476b382d50721357058","placeholder":"​","style":"IPY_MODEL_3ed5522d85964531a879c082012866a9","value":"config.json: 100%"}},"a16595e2883a445589a1410a659865af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ee6fb265f2f4ae3bb06a49ff0484819","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6888be7e9f044619c80ec1eed8d90c0","value":570}},"aee64cd2a2ca4cb0887f02df370a4f08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73bd37d0fc2a4705b478fd48aaef24bd","placeholder":"​","style":"IPY_MODEL_267004797b4d49f6becc12f43cc6b246","value":" 570/570 [00:00&lt;00:00, 48.3kB/s]"}},"3e5437c01143454aa6d751868f32324b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1429522ede914476b382d50721357058":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ed5522d85964531a879c082012866a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ee6fb265f2f4ae3bb06a49ff0484819":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6888be7e9f044619c80ec1eed8d90c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73bd37d0fc2a4705b478fd48aaef24bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"267004797b4d49f6becc12f43cc6b246":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a31cce936ed44a0e94cf732cea699419":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4a992e8b06c4b2599004d14c48e7127","IPY_MODEL_64dcf30234fb4e1c98409dabad1b9c57","IPY_MODEL_2d1eabc095af4b33a64da87b590d4a13"],"layout":"IPY_MODEL_768ec08a908540fa9d3c4f7b60e00c85"}},"f4a992e8b06c4b2599004d14c48e7127":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16f5f37b6bd64d5ba6f23290bbe400d1","placeholder":"​","style":"IPY_MODEL_84231cf2b0db4f00a71f5d24878b9a6d","value":"model.safetensors: 100%"}},"64dcf30234fb4e1c98409dabad1b9c57":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fbb9835c70df45bfb2d6b4b8c5a07afa","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ea4e3c3dfe645e98e3ba3824673ac95","value":440449768}},"2d1eabc095af4b33a64da87b590d4a13":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc49257937da4382abf261e081387234","placeholder":"​","style":"IPY_MODEL_acd89078cc364594b4fa28bc476a0665","value":" 440M/440M [00:02&lt;00:00, 248MB/s]"}},"768ec08a908540fa9d3c4f7b60e00c85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16f5f37b6bd64d5ba6f23290bbe400d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84231cf2b0db4f00a71f5d24878b9a6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbb9835c70df45bfb2d6b4b8c5a07afa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ea4e3c3dfe645e98e3ba3824673ac95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc49257937da4382abf261e081387234":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acd89078cc364594b4fa28bc476a0665":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Aller dans le répertoire sentiment_analysis sur Drive\n","%cd /content/drive/MyDrive/sentiment_analysis\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmsKsO89KIOv","executionInfo":{"status":"ok","timestamp":1736177433894,"user_tz":-60,"elapsed":1676,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"288bb85a-b589-4821-9dd2-aef20c2ce0f2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/sentiment_analysis\n"]}]},{"cell_type":"code","source":["# Création du fichier README.md\n","with open(\"README.md\", \"w\") as f:\n","    f.write(\"# sentiment_analysis\\n\\nAnalyse de sentiments sur Twitter.\\n\")\n"],"metadata":{"id":"TMYK5tX7PWQT","executionInfo":{"status":"ok","timestamp":1736177446580,"user_tz":-60,"elapsed":258,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Création du fichier .gitignore\n","with open(\".gitignore\", \"w\") as gitignore:\n","    gitignore.write(\"\"\"\\\n","# Ignorer les fichiers CSV et volumineux\n","*.csv\n","*.pkl\n","*.npz\n","\n","# Ignorer les checkpoints Jupyter Notebook\n",".ipynb_checkpoints/\n","\n","# Ignorer les caches Python\n","__pycache__/\n","*.py[cod]\n","\n","# Ignorer les fichiers temporaires ou de sauvegarde\n","*.tmp\n","*.log\n","*.bak\n","*.swp\n","~*\n","\n","# Ignorer les fichiers système\n",".DS_Store\n","\n","# Ignorer le dossier data/\n","data/\n","\"\"\")\n"],"metadata":{"id":"i3oBYeAlPY2H","executionInfo":{"status":"ok","timestamp":1736177462390,"user_tz":-60,"elapsed":423,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Initialiser le dépôt Git\n","!git init\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2zawqTpfPc4k","executionInfo":{"status":"ok","timestamp":1736177473488,"user_tz":-60,"elapsed":259,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"fcb7fade-ee0a-43a2-fd71-b818a90f56d7"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n","\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n","\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n","\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n","\u001b[33mhint: \u001b[m\n","\u001b[33mhint: \tgit branch -m <name>\u001b[m\n","Initialized empty Git repository in /content/drive/MyDrive/sentiment_analysis/.git/\n"]}]},{"cell_type":"code","source":["# Ajouter les fichiers au commit\n","!git add README.md .gitignore\n","\n","# Faire le commit\n","!git commit -m \"First commit: Ajout du README et du .gitignore\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBSLOsjXPfDO","executionInfo":{"status":"ok","timestamp":1736177482442,"user_tz":-60,"elapsed":454,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"7ef681a9-6ad3-4120-aa00-f69ebb34d285"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[master (root-commit) 57206c7] First commit: Ajout du README et du .gitignore\n"," 2 files changed, 27 insertions(+)\n"," create mode 100644 .gitignore\n"," create mode 100644 README.md\n"]}]},{"cell_type":"code","source":["# Configurer le nom d'utilisateur et l'adresse email\n","!git config --global user.name \"JulienDataScientist64\"\n","!git config --global user.email \"jcantalapiedra1@gmail.com\"\n"],"metadata":{"id":"XjQ3gAyvKEn9","executionInfo":{"status":"ok","timestamp":1736177494198,"user_tz":-60,"elapsed":243,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Créer le dossier .ssh\n","!mkdir -p ~/.ssh\n","\n","# Copier les clés SSH depuis Drive\n","!cp /content/drive/MyDrive/ssh_keys/id_rsa ~/.ssh/id_rsa\n","!cp /content/drive/MyDrive/ssh_keys/id_rsa.pub ~/.ssh/id_rsa.pub\n","\n","# Définir les permissions\n","!chmod 600 ~/.ssh/id_rsa\n","\n","# Ajouter GitHub aux hôtes connus\n","!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n","\n","# Vérifier la connexion SSH avec GitHub\n","!ssh -T git@github.com\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQOlhZwvKIWX","executionInfo":{"status":"ok","timestamp":1736177507066,"user_tz":-60,"elapsed":1119,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"746977fb-6993-48c8-ad09-7666511ec4fd"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["# github.com:22 SSH-2.0-f8a638b02\n","Hi JulienDataScientist64! You've successfully authenticated, but GitHub does not provide shell access.\n"]}]},{"cell_type":"code","source":["# Vérifie si un remote 'origin' existe déjà\n","!git remote -v\n","\n","# Si un remote 'origin' existe déjà et est incorrect, supprime-le\n","!git remote remove origin\n","\n","# Ajouter le remote correct\n","!git remote add origin git@github.com:JulienDataScientist64/sentiment_analysis.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VoztOIR0P3MQ","executionInfo":{"status":"ok","timestamp":1736177581550,"user_tz":-60,"elapsed":456,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"bfbe7604-c9bf-480e-8bcf-02b2c04f0f44"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["origin\tgit@github.com:JulienDataScientist64/sentiment_analysis.git (fetch)\n","origin\tgit@github.com:JulienDataScientist64/sentiment_analysis.git (push)\n"]}]},{"cell_type":"code","source":["!git branch -M main\n"],"metadata":{"id":"x_obdnaaKIdV","executionInfo":{"status":"ok","timestamp":1736177603767,"user_tz":-60,"elapsed":223,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["!git push -u origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmZyBtTKP_M8","executionInfo":{"status":"ok","timestamp":1736177614938,"user_tz":-60,"elapsed":1090,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"c806b796-b343-4484-e6ec-f71b22d42684"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Enumerating objects: 4, done.\n","Counting objects:  25% (1/4)\rCounting objects:  50% (2/4)\rCounting objects:  75% (3/4)\rCounting objects: 100% (4/4)\rCounting objects: 100% (4/4), done.\n","Delta compression using up to 8 threads\n","Compressing objects:  25% (1/4)\rCompressing objects:  50% (2/4)\rCompressing objects:  75% (3/4)\rCompressing objects: 100% (4/4)\rCompressing objects: 100% (4/4), done.\n","Writing objects:  25% (1/4)\rWriting objects:  50% (2/4)\rWriting objects:  75% (3/4)\rWriting objects: 100% (4/4)\rWriting objects: 100% (4/4), 537 bytes | 67.00 KiB/s, done.\n","Total 4 (delta 0), reused 0 (delta 0), pack-reused 0\n","To github.com:JulienDataScientist64/sentiment_analysis.git\n"," * [new branch]      main -> main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eri5e49-QMC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ajouter le notebook au commit\n","!git add notebook/sentiment_analysis_notebook.ipynb\n","\n","# Faire un commit avec un message descriptif\n","!git commit -m \"Ajout du notebook d'analyse de sentiments\"\n"],"metadata":{"id":"4OugbnjiQaoE","executionInfo":{"status":"ok","timestamp":1736177728411,"user_tz":-60,"elapsed":232,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"82983599-386e-4fea-c5ef-78feb74d72b8","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: pathspec 'notebook/sentiment_analysis_notebook.ipynb' did not match any files\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mnotebook/\u001b[m\n","\t\u001b[31msrc/\u001b[m\n","\n","nothing added to commit but untracked files present (use \"git add\" to track)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"vaQAKt9MQarf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"x2LNmdpOQaxP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Réalisez une analyse de sentiments grâce au Deep Learning**  \n","---\n","# **Contexte**\n","\n","Je suis ingénieur IA chez **MIC (Marketing Intelligence Consulting)**, un cabinet spécialisé dans les solutions de marketing digital basées sur l’intelligence artificielle. Récemment, nous avons été missionnés par **Air Paradis**, une compagnie aérienne, pour les aider à améliorer leur image sur les réseaux sociaux. Leur problématique est claire : ils souhaitent anticiper et limiter les **bad buzz** en identifiant rapidement les tweets à sentiment négatif.\n","\n","Pour répondre à leur besoin, mon objectif est de développer un **prototype d’IA** capable de prédire le sentiment d’un tweet, c'est-à-dire déterminer s'il exprime un sentiment **positif** ou **négatif**. Ce prototype sera également **déployé sur le Cloud**, afin de permettre une intégration facile dans leurs systèmes internes.\n","\n","---\n","\n","# **Problématique**\n","\n","Comment concevoir une solution IA performante et facilement déployable, capable de prédire avec précision le sentiment associé à un tweet, tout en intégrant une démarche **MLOps complète** pour garantir le suivi et l’amélioration continue du modèle en production ?\n","\n","L’objectif n’est pas seulement de répondre à la demande spécifique d’Air Paradis, mais aussi de proposer une solution généralisable à d’autres cas d’usage dans le domaine du marketing digital.\n","\n","---\n","\n","# **Objectif du projet**\n","\n","1. **Construire un prototype fonctionnel** :\n","   - Créer un modèle capable de prédire le sentiment d’un tweet.\n","   - Explorer plusieurs approches, des modèles classiques aux modèles avancés de Deep Learning, y compris l’utilisation de **BERT**.\n","\n","2. **Déployer une API Cloud** :\n","   - Fournir une interface accessible pour tester et utiliser le modèle en production.\n","\n","3. **Adopter une démarche orientée MLOps** :\n","   - Assurer le **suivi des expérimentations** avec MLFlow.\n","   - Mettre en œuvre un pipeline de **déploiement continu** avec des tests automatisés.\n","   - Intégrer un système de **suivi en production**, avec la possibilité de tracer les prédictions incorrectes et de générer des alertes en cas d’anomalies.\n","\n","---\n","\n","Avec cette mission, je vais démontrer non seulement ma capacité à construire des modèles performants, mais aussi à intégrer une méthodologie MLOps robuste, essentielle pour la mise en production des modèles IA dans un environnement réel.\n"],"metadata":{"id":"qC9g_hqzMf6V"}},{"cell_type":"markdown","source":["# 1. Préparation et Exploration des Données\n","- **Importer les données Open Source** : Télécharge et charge les données dans un DataFrame Pandas.\n","- **Exploration des Données (EDA)** :\n","  - Analyser les distributions des sentiments, les heures de publication, etc.\n","  - Identifier les caractéristiques clés (longueur des tweets, utilisation de hashtags/mentions, etc.).\n","- **Nettoyage des données** : Utilise le processus de nettoyage que nous avons déjà élaboré.\n","\n","# 2. Développement des Modèles de Prédiction de Sentiment\n","### Approche 1 : Modèle sur Mesure Simple\n","- **Pré-traitement des Données** : Utilise des techniques de transformation textuelle comme TF-IDF pour vectoriser les tweets.\n","- **Modèle Classique** : Implémente une régression logistique (ou un autre modèle de machine learning classique) pour prédire le sentiment.\n","- **Évaluation du Modèle** : Utilise des métriques comme l'Accuracy, le F1-score, et l'AUC-ROC pour évaluer les performances.\n","\n","### Approche 2 : Modèle sur Mesure Avancé\n","- **Pré-traitement Avancé** : Implémente des embeddings de mots (Word2Vec, GloVe, ou FastText) pour les tweets.\n","- **Modèle de Réseaux de Neurones Profonds** : Crée un modèle de réseau de neurones avec TensorFlow/Keras, utilisant des embeddings comme couche d'entrée.\n","- **Essai de Différents Embeddings** : Compare les performances de plusieurs embeddings et conserve le meilleur.\n","- **Expérimentation avec BERT** : Utilise un modèle pré-entraîné comme BERT (par exemple, avec transformers de Hugging Face) pour comparer les performances avec les embeddings plus simples.\n","- **Évaluation** : Compare les performances des différentes approches pour choisir la meilleure.\n","\n","# 3. Gestion des Expérimentations avec MLFlow\n","- **Installation et Configuration de MLFlow** : Installe MLFlow et configure un tracking server local ou distant.\n","- **Tracking des Expériences** :\n","  - Suis les hyperparamètres, les performances des modèles, et les versions des données.\n","  - Log les modèles entraînés pour une gestion efficace.\n","- **Reporting avec MLFlow** : Génère des rapports détaillés pour chaque expérience.\n","\n","# 4. Déploiement du Modèle via une API\n","- **Création de l’API** : Utilise Flask ou FastAPI pour créer une API qui reçoit un tweet, l’analyse, et renvoie la prédiction de sentiment.\n","- **Déploiement sur une Solution Cloud Gratuite** :\n","  - **Options** : Azure Web App (gratuit ASP F1), Heroku (avec plan étudiant), ou PythonAnywhere.\n","  - Déploie l’API et vérifie qu’elle fonctionne correctement.\n","\n","# 5. Mise en Œuvre d'une Démarche MLOps\n","- **Présentation des Principes de MLOps** : Prépare un support expliquant les concepts clés de MLOps (automatisation, traçabilité, monitoring).\n","- **Pipeline de Déploiement Continu** :\n","  - Utilise Git + GitHub pour la gestion du code source.\n","  - Configure des tests unitaires pour le code d’entraînement et le code de l’API.\n","  - Intègre un pipeline de CI/CD pour automatiser le déploiement (GitHub Actions, par exemple).\n","\n","# 6. Suivi de la Performance du Modèle en Production\n","- **Configurer Azure Application Insights** : Crée un service Azure Application Insights pour le monitoring.\n","- **Remonter les Anomalies** :\n","  - Log les tweets mal prédits et les erreurs associées.\n","  - Déclenche une alerte (SMS ou email) si plus de 3 tweets sont mal prédits en 5 minutes.\n","- **Analyse et Amélioration Continue** : Propose une méthode pour analyser les statistiques et améliorer le modèle dans le temps.\n","\n","# 7. Rédaction de l'Article pour le Blog\n","- **Contenu de l'Article** :\n","  - Explique la méthodologie utilisée (modélisation, MLOps).\n","  - Présente les résultats des expérimentations.\n","  - Détaille les bénéfices de l’approche MLOps pour assurer la fiabilité et la maintenabilité des modèles d’IA.\n","\n","# Ressources à Utiliser\n","- **Modèles et Bibliothèques** :\n","  - scikit-learn pour les modèles classiques.\n","  - TensorFlow/Keras pour les réseaux de neurones.\n","  - transformers de Hugging Face pour BERT.\n","  - MLFlow pour la gestion des expérimentations.\n","  - Flask ou FastAPI pour l'API.\n","- **Services Cloud Gratuits** :\n","  - Azure : Déploiement d'API avec Azure Web App (plan gratuit).\n","  - Heroku : Déploiement d'API avec le package étudiant.\n","\n","# Plan d'Action\n","1. **Étape 1** : Nettoyage et exploration des données.\n","2. **Étape 2** : Implémentation et évaluation des modèles.\n","3. **Étape 3** : Gestion des expérimentations avec MLFlow.\n","4. **Étape 4** : Déploiement de l'API et configuration de MLOps.\n","5. **Étape 5** : Monitoring et suivi des performances.\n","6. **Étape 6** : Rédaction de l'article pour le blog.\n"],"metadata":{"id":"uZEeG5ZLRqc3"}},{"cell_type":"code","source":["import pandas as pd\n","import warnings\n","from IPython.display import display, Markdown\n","import numpy as np\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","# Configuration des options de pandas pour l'affichage\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.max_rows', None)\n","pd.options.display.float_format = '{:,.2f}'.format\n","pd.options.mode.chained_assignment = None\n","\n","# Gestion des avertissements\n","warnings.filterwarnings('ignore', category=DeprecationWarning)\n","\n","def display_info(key, value):\n","    \"\"\" Affiche les informations dans un format markdown. \"\"\"\n","    display(Markdown(f\"**{key}:** {value}\"))\n","\n","# Fonction de séparation pour affichage clair\n","def display_separator():\n","    \"\"\" Affiche un séparateur markdown. \"\"\"\n","    display(Markdown(\"-\" * 80))\n","\n","# Fonction pour afficher le docstring\n","def display_docstring(func):\n","    display(Markdown(f\"**Documentation for {func.__name__}:**\\n\\n{func.__doc__}\"))\n"],"metadata":{"id":"52fOF1f4-ure"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"M24vtr9wc4p0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736156095329,"user_tz":-60,"elapsed":19594,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"e8ff68de-8aa0-4e7a-e21e-f3817a9ac74a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Generer une clef SSh pour GitHub\n","!ssh-keygen -t rsa -b 4096 -C \"jcantalapiedra1@gmail.com\" -f ~/.ssh/id_rsa -N \"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qdb6ospUnWaH","executionInfo":{"status":"ok","timestamp":1732778172331,"user_tz":-60,"elapsed":3632,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"53da4a86-8132-463a-87cf-7f8f26dd2215"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating public/private rsa key pair.\n","Your identification has been saved in /root/.ssh/id_rsa\n","Your public key has been saved in /root/.ssh/id_rsa.pub\n","The key fingerprint is:\n","SHA256:YuKSGw1yvGc/HFxugpx3ExvaOurPORK0f8RokL/q1HM jcantalapiedra1@gmail.com\n","The key's randomart image is:\n","+---[RSA 4096]----+\n","|                 |\n","|                 |\n","|     .           |\n","| .  +   +        |\n","|. +o.BoBS+       |\n","| o **+X.X        |\n","|  = **oOE.       |\n","|   B.oO+.        |\n","|  .o====         |\n","+----[SHA256]-----+\n"]}]},{"cell_type":"code","source":["# Afficher ka clef SSh pour la copier dans Git\n","!cat ~/.ssh/id_rsa.pub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yovHJCTxnWfG","executionInfo":{"status":"ok","timestamp":1732778185710,"user_tz":-60,"elapsed":188,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"d8de7bbd-b954-4f40-d6b1-04801f26e2ef"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDFFba19ViPyPqUZGQbjJCXHsot0BtVA7/+QZyDaGUTMvCJ/VQn8Dr7NeteG/gTegNcGXu6+x2WgVlupIHVdj+VGH3tq0whYlh0OYpgv+fy6FcDvXV9xv7EwbClkwI9Bm0VWlzgjoPnuPrDpTgRAmdW5lb0h1bqJx+i6Am2eXhneyxHGtcrl0W8zPSFQF4oOKnFNcBv6P4/A/jnlNgABOX7Tx3uPoFr0fH3hJiHCryXPUhKCKfsZp/1Hvnn8D2MS5bRVgEXI90zx0vGuRy1uFokbaXQcHVQ8nciZ38/MnPD6kPlWgePI/FGY3RsMeNMIbZ4WQN/VwyOgAGi7FYvDi7c+iuQmI5bl0LQ6Cngu7PQEeZj0tK1M8FcnXIdCpDCHT9hwEhwPdwbicerUOJceXEJKt8KzV9bXz4uwGK+cA/PXwvyCUduR75vXskMzzY30dMJD2/DlqvKfWvr0pCWCJZlG3/ThfwZb9c3pNvuTlLF39ukYl1F7tcD6Tq2oFXT6Rxj5k4gjhAsg9/l6d8Ny0WkPv6+V21mDKCy+z8iGm2uyl2wDJbhdKDOoNGQ6DtWyheJhpAhUPrZivI7ORb2YKG53Etn1LqZzTVK8Ku6bSGEC9lp61MhsNVr1AhRvejZhYO0lmIFyFhY3U4rRCSiScqnuyJsomxg3UgueSqyhrZgew== jcantalapiedra1@gmail.com\n"]}]},{"cell_type":"code","source":["# Créer le dossier sur mon drive avec copie des clefs\n","!mkdir -p /content/drive/MyDrive/ssh_keys  # Créer le dossier ssh_keys sur Google Drive\n","!cp ~/.ssh/id_rsa /content/drive/MyDrive/ssh_keys/id_rsa  # Copier la clé privée\n","!cp ~/.ssh/id_rsa.pub /content/drive/MyDrive/ssh_keys/id_rsa.pub  # Copier la clé publique"],"metadata":{"id":"Rs6a-4YXoZHo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A chaque redemarrage de session colab\n","!mkdir -p ~/.ssh  # Créer le répertoire SSH\n","!cp /content/drive/MyDrive/ssh_keys/id_rsa ~/.ssh/id_rsa  # Copier la clé privée\n","!cp /content/drive/MyDrive/ssh_keys/id_rsa.pub ~/.ssh/id_rsa.pub  # Copier la clé publique\n","!chmod 600 ~/.ssh/id_rsa  # Appliquer les permissions correctes à la clé privée\n","!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts  # Ajouter GitHub aux hôtes connus\n","!ssh -T git@github.com  # Tester la connexion SSH"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nUjQdsweoy8W","executionInfo":{"status":"ok","timestamp":1732889837171,"user_tz":-60,"elapsed":1520,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"74daaa0b-5785-4c0b-db68-09d8742249c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["# github.com:22 SSH-2.0-babeld-9e2e2a76e\n","Hi JulienDataScientist64! You've successfully authenticated, but GitHub does not provide shell access.\n"]}]},{"cell_type":"markdown","source":["# 1. Préparation et Exploration des Données"],"metadata":{"id":"FTVmL_iWqXz0"}},{"cell_type":"code","source":["# Définition du chemin des données\n","file_path =  '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/training.csv'\n","data = pd.read_csv(file_path, encoding='latin1')\n","data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":310},"id":"ata2_GR2Al0U","executionInfo":{"status":"ok","timestamp":1731251586960,"user_tz":-60,"elapsed":5182,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"ad536413-5a9a-4093-c821-163448f52083","collapsed":true},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n","0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n","1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n","2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n","3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n","4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n","\n","  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n","0  is upset that he can't update his Facebook by ...                                                                   \n","1  @Kenichan I dived many times for the ball. Man...                                                                   \n","2    my whole body feels itchy and like its on fire                                                                    \n","3  @nationwideclass no, it's not behaving at all....                                                                   \n","4                      @Kwesidei not the whole crew                                                                    "],"text/html":["\n","  <div id=\"df-8c00b49f-f4e7-40fd-b73d-fe73a9dc355f\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1467810369</th>\n","      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n","      <th>NO_QUERY</th>\n","      <th>_TheSpecialOne_</th>\n","      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1467810672</td>\n","      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>scotthamilton</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1467810917</td>\n","      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mattycus</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1467811184</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>ElleCTF</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1467811193</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Karoli</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1467811372</td>\n","      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>joy_wolf</td>\n","      <td>@Kwesidei not the whole crew</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c00b49f-f4e7-40fd-b73d-fe73a9dc355f')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-8c00b49f-f4e7-40fd-b73d-fe73a9dc355f button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-8c00b49f-f4e7-40fd-b73d-fe73a9dc355f');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-e5e3951e-8112-4e4e-81a9-1b3bc42564d2\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5e3951e-8112-4e4e-81a9-1b3bc42564d2')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-e5e3951e-8112-4e4e-81a9-1b3bc42564d2 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"data"}},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\n","data.columns = DATASET_COLUMNS\n","data.head()  # Affiche les premières lignes des données\n","data.info()  # Affiche la structure des données"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LM2Nh7u3jXzL","executionInfo":{"status":"ok","timestamp":1731251587318,"user_tz":-60,"elapsed":360,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"85217559-bb29-4686-8802-17f2abda63f1","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1599999 entries, 0 to 1599998\n","Data columns (total 6 columns):\n"," #   Column     Non-Null Count    Dtype \n","---  ------     --------------    ----- \n"," 0   target     1599999 non-null  int64 \n"," 1   ids        1599999 non-null  int64 \n"," 2   date       1599999 non-null  object\n"," 3   flag       1599999 non-null  object\n"," 4   user       1599999 non-null  object\n"," 5   TweetText  1599999 non-null  object\n","dtypes: int64(2), object(4)\n","memory usage: 73.2+ MB\n"]}]},{"cell_type":"markdown","source":["## Analyse Préliminaire et Compréhension des Données"],"metadata":{"id":"J66nlSOxBZ9B"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from wordcloud import WordCloud\n","from sklearn.feature_extraction.text import CountVectorizer\n","import networkx as nx\n","from itertools import combinations\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import warnings\n","\n","# Configuration\n","warnings.filterwarnings('ignore')\n","nltk.download('stopwords')\n","\n","class SentimentEDA:\n","    def __init__(self, file_path):\n","        self.file_path = file_path\n","        self.data = None\n","        self.stop_words = set(stopwords.words('english'))\n","        self.stemmer = PorterStemmer()\n","\n","    def load_data(self):\n","        \"\"\"Charger et préparer les données.\"\"\"\n","        print(\"Chargement des données...\")\n","        DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\n","        self.data = pd.read_csv(self.file_path, encoding='latin1')\n","        self.data.columns = DATASET_COLUMNS\n","        self.data.drop(['ids', 'flag', 'user'], axis=1, inplace=True)\n","        self.data.drop_duplicates(subset='TweetText', keep='first', inplace=True)\n","        print(f\"Données chargées : {self.data.shape[0]} lignes après suppression des doublons.\")\n","\n","    def clean_tweet(self, tweet):\n","        \"\"\"Nettoyer un tweet.\"\"\"\n","        tweet = re.sub(r\"@\\w+\", \"\", tweet)\n","        tweet = re.sub(r\"http\\S+\", \"\", tweet)\n","        tweet = re.sub(r\"[^a-zA-Z\\s]\", \" \", tweet)\n","        tweet = re.sub(r\"\\s+\", \" \", tweet).strip()\n","        words = tweet.lower().split()\n","        words = [word for word in words if word not in self.stop_words]\n","        return ' '.join([self.stemmer.stem(word) for word in words])\n","\n","    def clean_tweets(self):\n","        \"\"\"Appliquer le nettoyage à tous les tweets.\"\"\"\n","        print(\"Nettoyage des tweets en cours...\")\n","        self.data['Clean_TweetText'] = self.data['TweetText'].apply(self.clean_tweet)\n","        print(\"Nettoyage terminé.\")\n","\n","    def analyze_tweet_lengths(self):\n","        \"\"\"Analyser les longueurs des tweets avant et après nettoyage.\"\"\"\n","        self.data['Original_Length'] = self.data['TweetText'].str.len()\n","        self.data['Cleaned_Length'] = self.data['Clean_TweetText'].str.len()\n","\n","        plt.figure(figsize=(12, 6))\n","        plt.hist(self.data['Original_Length'], bins=30, alpha=0.6, label='Avant nettoyage')\n","        plt.hist(self.data['Cleaned_Length'], bins=30, alpha=0.6, label='Après nettoyage')\n","        plt.title(\"Distribution des longueurs des tweets\")\n","        plt.xlabel(\"Longueur\")\n","        plt.ylabel(\"Fréquence\")\n","        plt.legend()\n","        plt.show()\n","\n","    def analyze_time_distribution(self):\n","        \"\"\"Analyser la distribution des tweets par jour et heure.\"\"\"\n","        if 'date' not in self.data.columns:\n","            print(\"Les dates ne sont pas disponibles pour cette analyse.\")\n","            return\n","\n","        self.data['date'] = pd.to_datetime(self.data['date'], errors='coerce')\n","        self.data['DayOfWeek'] = self.data['date'].dt.day_name()\n","        self.data['Hour'] = self.data['date'].dt.hour\n","\n","        plt.figure(figsize=(12, 6))\n","        self.data.groupby('DayOfWeek')['target'].value_counts().unstack().plot(kind='bar', stacked=True)\n","        plt.title(\"Distribution des tweets par jour de la semaine\")\n","        plt.xlabel(\"Jour de la semaine\")\n","        plt.ylabel(\"Nombre de tweets\")\n","        plt.show()\n","\n","        plt.figure(figsize=(12, 6))\n","        self.data.groupby('Hour')['target'].value_counts().unstack().plot(kind='bar', stacked=True)\n","        plt.title(\"Distribution des tweets par heure\")\n","        plt.xlabel(\"Heure\")\n","        plt.ylabel(\"Nombre de tweets\")\n","        plt.show()\n","\n","    def display_sentiment_counts(self):\n","        \"\"\"Afficher les chiffres précis des tweets positifs et négatifs.\"\"\"\n","        positive_count = self.data[self.data['target'] == 4].shape[0]\n","        negative_count = self.data[self.data['target'] == 0].shape[0]\n","        print(f\"Nombre de tweets positifs : {positive_count}\")\n","        print(f\"Nombre de tweets négatifs : {negative_count}\")\n","\n","    def analyze_sentiment_frequency(self):\n","        \"\"\"Analyser la fréquence des sentiments.\"\"\"\n","        sentiment_counts = self.data['target'].value_counts()\n","        sentiment_counts.plot(kind='pie', autopct='%1.1f%%', labels=[\"Négatif\", \"Positif\"], startangle=140)\n","        plt.title(\"Répartition des sentiments\")\n","        plt.ylabel(\"\")\n","        plt.show()\n","\n","    def plot_target_distribution(self):\n","        \"\"\"Visualiser la distribution des cibles.\"\"\"\n","        target_counts = self.data['target'].value_counts()\n","        plt.figure(figsize=(6, 4))\n","        target_counts.plot(kind='bar', color=['red', 'blue'], edgecolor='black')\n","        plt.title(\"Distribution du Nombre de Tweets par Cible\")\n","        plt.xlabel(\"Cible\")\n","        plt.ylabel(\"Nombre de Tweets\")\n","        plt.xticks(ticks=[0, 1], labels=[\"Négatif (0)\", \"Positif (4)\"], rotation=0)\n","        plt.show()\n","\n","        # EDA : Analyse de la Fréquence des Mots\n","    def plot_top_words(self, target, title, color):\n","        \"\"\"Visualiser les mots les plus fréquents pour une cible spécifique.\"\"\"\n","        # Filtrer les tweets en fonction de la cible\n","        words = ' '.join(self.data[self.data['target'] == target]['Clean_TweetText']).split()\n","        word_counts = Counter(words)\n","        common_words = word_counts.most_common(20)\n","        words, counts = zip(*common_words)\n","        plt.figure(figsize=(10, 5))\n","        plt.bar(words, counts, color=color)\n","        plt.title(title)\n","        plt.xlabel(\"Mots\")\n","        plt.ylabel(\"Fréquence\")\n","        plt.xticks(rotation=45)\n","        plt.show()\n","\n","    def generate_wordcloud(self, target, title, background_color, colormap):\n","        \"\"\"Générer un WordCloud pour une cible spécifique.\"\"\"\n","        # Filtrer les tweets en fonction de la cible\n","        text = ' '.join(self.data[self.data['target'] == target]['Clean_TweetText'])\n","        wordcloud = WordCloud(\n","            width=800,\n","            height=400,\n","            background_color=background_color,\n","            colormap=colormap\n","        ).generate(text)\n","\n","        # Affichage du WordCloud\n","        plt.figure(figsize=(10, 5))\n","        plt.imshow(wordcloud, interpolation='bilinear')\n","        plt.title(title)\n","        plt.axis('off')\n","        plt.show()\n","\n","    def plot_cooccurrence_network(self, target, num_nodes=20):\n","        \"\"\"Tracer le graphe des co-occurrences pour une cible spécifique.\"\"\"\n","        # Filtrer les tweets en fonction de la cible\n","        tweets = self.data[self.data['target'] == target]['Clean_TweetText']\n","\n","        # Création du graphe de co-occurrence\n","        cooccurrence_graph = nx.Graph()\n","        for tweet in tweets:\n","            words = tweet.split()\n","            for pair in combinations(words, 2):  # Toutes les paires de mots\n","                if cooccurrence_graph.has_edge(pair[0], pair[1]):\n","                    cooccurrence_graph[pair[0]][pair[1]]['weight'] += 1\n","                else:\n","                    cooccurrence_graph.add_edge(pair[0], pair[1], weight=1)\n","\n","        # Identifier les nœuds les plus connectés\n","        degrees = dict(cooccurrence_graph.degree(weight='weight'))\n","        top_nodes = sorted(degrees, key=degrees.get, reverse=True)[:num_nodes]\n","        subgraph = cooccurrence_graph.subgraph(top_nodes)\n","\n","        # Dessiner le graphe\n","        plt.figure(figsize=(12, 8))\n","        pos = nx.spring_layout(subgraph, seed=42)  # Pour un placement reproductible\n","        nx.draw(\n","            subgraph,\n","            pos,\n","            with_labels=True,\n","            node_size=3000,\n","            node_color='skyblue',\n","            font_size=10,\n","            font_weight='bold'\n","        )\n","        plt.title(f\"Graphe des Co-occurrences ({'Positifs' if target == 4 else 'Négatifs'})\")\n","        plt.show()\n","\n","\n","    def perform_eda(self):\n","        \"\"\"Réaliser l'EDA complet.\"\"\"\n","        print(\"Début de l'EDA...\")\n","        self.analyze_tweet_lengths()\n","        self.analyze_time_distribution()\n","        self.display_sentiment_counts()\n","        self.analyze_sentiment_frequency()\n","        self.plot_target_distribution()\n","        self.plot_top_words(4, \"Top 20 Mots Fréquents (Positifs)\", 'blue')\n","        self.plot_top_words(0, \"Top 20 Mots Fréquents (Négatifs)\", 'red')\n","        self.generate_wordcloud(4, \"WordCloud des Tweets Positifs\", \"white\", \"Blues\")\n","        self.generate_wordcloud(0, \"WordCloud des Tweets Négatifs\", \"black\", \"Reds\")\n","        self.plot_cooccurrence_network(4)\n","        self.plot_cooccurrence_network(0)\n","        print(\"EDA terminé.\")\n","\n","\n","    def save_cleaned_data(self, output_path):\n","        \"\"\"Sauvegarder les données nettoyées.\"\"\"\n","        self.data.to_csv(output_path, index=False)\n","        print(f\"Données nettoyées enregistrées dans {output_path}.\")\n","\n","\n","\n","if __name__ == \"__main__\":\n","    eda = SentimentEDA(file_path='/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/training.csv')\n","    eda.load_data()\n","    eda.clean_tweets()\n","    eda.perform_eda()\n","    #eda.save_cleaned_data('/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/tweets_cleaned.csv')\n"],"metadata":{"id":"Fj8q8A5F8neQ","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1tRGBgihFcLlm0APtBUysLP-2i1yF06Yl"},"executionInfo":{"status":"ok","timestamp":1732892312828,"user_tz":-60,"elapsed":297774,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"0bd08b07-e29d-435b-8f8e-cf4ddaed6e62"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["* Structure des Données\n","sentiment : Label binaire (0 = négatif, 4 = positif).\n","id : Identifiant unique du tweet.\n","date : Date de publication du tweet.\n","query : \"NO_QUERY\" pour tous les tweets, donc probablement inutile.\n","user : Nom d'utilisateur qui a posté le tweet.\n","text : Contenu du tweet."],"metadata":{"id":"Aw9zFWnxAzwb"}},{"cell_type":"markdown","source":["* Analyse des Exemples d'Identifiants Non Uniques\n","Nous avons plusieurs cas où le même identifiant (id) est associé à des tweets identiques, mais avec des labels de sentiment différents (0 pour négatif et 4 pour positif). Voici quelques observations :\n","\n","Même Texte, Différents Labels : Pour chaque exemple, le texte du tweet est identique, mais le label de sentiment varie entre 0 et 4. Cela indique une incohérence possible dans l'étiquetage des données.\n","Utilisateur Identique : L'utilisateur ayant posté le tweet est également le même pour les deux entrées. Cela renforce l'idée que ces tweets sont des duplications avec des labels de sentiment discordants."],"metadata":{"id":"x22E90NMCAS4"}},{"cell_type":"markdown","source":["# Développement des Modèles de Prédiction"],"metadata":{"id":"rSRYxvipqcnc"}},{"cell_type":"markdown","source":["## Pour faire le commit du jour"],"metadata":{"id":"01jxIb81hFsr"}},{"cell_type":"code","source":["# Monter Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Configurer SSH\n","!mkdir -p ~/.ssh\n","!cp /content/drive/MyDrive/ssh_keys/id_rsa ~/.ssh/id_rsa\n","!cp /content/drive/MyDrive/ssh_keys/id_rsa.pub ~/.ssh/id_rsa.pub\n","!chmod 600 ~/.ssh/id_rsa\n","!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n","!ssh -T git@github.com\n","\n","# Configurer Git (Nom et email)\n","!git config --global user.name \"JulienDataScientist64\"\n","!git config --global user.email \"jcantalapiedra1@gmail.com\"\n","\n","# Naviguer dans le répertoire de ton projet (cloné ou existant dans Colab)\n","%cd /content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning\n","\n","# Synchroniser avec les changements distants\n","!git pull origin main --allow-unrelated-histories\n","\n","# Vérifier l'état des modifications\n","!git status\n","\n","# Ajouter toutes les modifications\n","!git add .\n","\n","# Créer un commit avec une description claire\n","!git commit -m \"Description des changements du jour\"\n","\n","# Pousser les modifications vers GitHub\n","!git push origin main\n"],"metadata":{"id":"VaNDxZYfgTH0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Procédure complète pour un nouveau projet dans Colab"],"metadata":{"id":"zzsDbIAtHdFW"}},{"cell_type":"code","source":["#Procédure complète pour un nouveau projet dans Colab#\n","from google.colab import drive\n","import os\n","\n","# Étape 1 : Monter Google Drive\n","drive.mount('/content/drive')\n","\n","# Étape 2 : Configurer SSH\n","!mkdir -p ~/.ssh\n","!cp /content/drive/MyDrive/ssh_keys/id_rsa ~/.ssh/id_rsa\n","!cp /content/drive/MyDrive/ssh_keys/id_rsa.pub ~/.ssh/id_rsa.pub\n","!chmod 600 ~/.ssh/id_rsa\n","!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n","!ssh -T git@github.com\n","\n","# Étape 3 : Configurer Git (Nom et email)\n","!git config --global user.name \"JulienDataScientist64\"\n","!git config --global user.email \"jcantalapiedra1@gmail.com\"\n","\n","# Étape 4 : Créer un répertoire pour le projet\n","new_project_path = '/content/drive/MyDrive/mon_nouveau_projet'\n","os.makedirs(new_project_path, exist_ok=True)\n","%cd {new_project_path}\n","\n","# Étape 5 : Créer un fichier README.md\n","with open(\"README.md\", \"w\") as f:\n","    f.write(\"# Mon Nouveau Projet\\n\\nDescription de mon projet.\")\n","\n","# Étape 6 : Initialiser Git dans le répertoire\n","!git init\n","\n","# Étape 7 : Créer un fichier .gitignore\n","with open(\".gitignore\", \"w\") as gitignore:\n","    gitignore.write(\"\"\"\n","training.csv\n","*.csv\n","__pycache__/\n","*.pkl\n",".DS_Store\n","config.json\n","\n","# Ignorer les fichiers PNG\n","*.png\n","\n","# Ignorer les fichiers temporaires ou de sauvegarde\n","*.tmp\n","*.log\n","*.bak\n","*.swp\n","~*\n","->>rajouter pour ignorer data set et touts les fichier volumineux sans importance\n","# Ignorer les sorties de Jupyter Notebook\n",".ipynb_checkpoints/\n","\"\"\")\n","\n","# Étape 8 : Ajouter et commiter les fichiers\n","!git add .\n","!git commit -m \"Initial commit: ajout du README et configuration initiale\"\n","\n","# Étape 9 : Lier le dépôt GitHub (remplace par le lien de ton dépôt GitHub)\n","!git remote add origin git@github.com:JulienDataScientist64/mon_nouveau_projet.git\n","\n","# Étape 10 : Pousser les modifications vers GitHub\n","!git push -u origin main\n"],"metadata":{"id":"Eh21LdhWdoP3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Pre Processing"],"metadata":{"id":"4JsTNogDO_N2"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Chemin vers le fichier d'entrée\n","FILE_PATH = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/training.csv'\n","\n","# Nom des colonnes du dataset\n","DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\n","\n","# Chargement du dataset avec le bon encodage\n","data = pd.read_csv(FILE_PATH, encoding='latin1', names=DATASET_COLUMNS)\n","\n","# Suppression des doublons pour éviter les répétitions\n","data.drop_duplicates(subset=\"TweetText\", inplace=True)\n","\n","# Conversion des labels : remplacer le label 4 par 1 (sentiment positif)\n","data['target'] = data['target'].replace(4, 1)\n","\n","# **Étape 1 : Création du dataset de test équilibré**\n","# Échantillonnage de 2500 tweets négatifs et 2500 tweets positifs\n","test_negatives = data[data['target'] == 0].sample(2500, random_state=42)\n","test_positives = data[data['target'] == 1].sample(2500, random_state=42)\n","\n","# Concaténation et mélange aléatoire des échantillons pour le dataset de test\n","hidden_data = pd.concat([test_negatives, test_positives]).sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Sauvegarde du dataset de test\n","hidden_data_PATH = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/test_data.csv'\n","hidden_data.to_csv(hidden_data_PATH, index=False)\n","\n","# Suppression des tweets de test du dataset principal\n","data = data[~data['TweetText'].isin(hidden_data['TweetText'])]\n","\n","# **Étape 2 : Création d'un dataset équilibré pour l'entraînement**\n","# Échantillonnage équilibré de 100,000 tweets pour chaque classe\n","negatives = data[data['target'] == 0].sample(100000, random_state=42)\n","positives = data[data['target'] == 1].sample(100000, random_state=42)\n","\n","# Concaténation et mélange aléatoire des échantillons pour l'entraînement\n","balanced_data = pd.concat([negatives, positives]).sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","# Sauvegarde du dataset équilibré d'entraînement\n","TRAIN_DATA_PATH = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/train_data.csv'\n","balanced_data.to_csv(TRAIN_DATA_PATH, index=False)\n","\n","# **Vérifications des données**\n","# Taille et répartition des classes dans le dataset hidden_data\n","print(\"Taille des données hidden_data :\", hidden_data.shape)\n","print(\"Répartition des classes dans le dataset hidden_data :\\n\", hidden_data['target'].value_counts())\n","\n","# Taille et répartition des classes dans le dataset d'entraînement\n","print(\"\\nTaille des données d'entraînement :\", balanced_data.shape)\n","print(\"Répartition des classes dans le dataset d'entraînement :\\n\", balanced_data['target'].value_counts())\n","\n","# Aperçu des données\n","print(\"\\nAperçu des données hidden_data :\")\n","print(hidden_data.head())\n","print(\"\\nColonnes du DataFrame d'entraînement :\", balanced_data.columns)\n","print(\"\\nAperçu des données d'entraînement :\")\n","print(balanced_data.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGg4Tk3hyJfD","executionInfo":{"status":"ok","timestamp":1732688754337,"user_tz":-60,"elapsed":5898,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"07eb56ae-1f89-43b3-9411-d9d88f7518c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Taille des données de test : (5000, 6)\n","Répartition des classes dans le dataset de test :\n"," target\n","0    2500\n","1    2500\n","Name: count, dtype: int64\n","\n","Taille des données d'entraînement : (200000, 6)\n","Répartition des classes dans le dataset d'entraînement :\n"," target\n","1    100000\n","0    100000\n","Name: count, dtype: int64\n","\n","Aperçu des données de test :\n","   target         ids                          date      flag           user  \\\n","0       0  2255153540  Sat Jun 20 10:50:18 PDT 2009  NO_QUERY  Charmed231281   \n","1       1  1881316460  Fri May 22 04:18:01 PDT 2009  NO_QUERY      natneagle   \n","2       1  2056281230  Sat Jun 06 10:51:32 PDT 2009  NO_QUERY        HippyDi   \n","3       0  1556845050  Sun Apr 19 00:37:54 PDT 2009  NO_QUERY      stephyang   \n","4       0  1833915956  Mon May 18 00:55:46 PDT 2009  NO_QUERY   anbudan_BALA   \n","\n","                                           TweetText  \n","0        Today i have night work at the airport fun   \n","1                  @ra1ne yea  so glad you liked it!  \n","2        @orangy68 Thats really sweet...back at you   \n","3  Holi show was fun.. after party at ten pin was...  \n","4  @peyarili I may not agree with certain things ...  \n","\n","Colonnes du DataFrame d'entraînement : Index(['target', 'ids', 'date', 'flag', 'user', 'TweetText'], dtype='object')\n","\n","Aperçu des données d'entraînement :\n","   target         ids                          date      flag            user  \\\n","0       1  2187967676  Mon Jun 15 21:05:06 PDT 2009  NO_QUERY       _LLcoolV_   \n","1       0  2067122804  Sun Jun 07 11:47:18 PDT 2009  NO_QUERY    Purple_C_A_T   \n","2       1  1828046218  Sun May 17 11:58:54 PDT 2009  NO_QUERY     SoniaWilson   \n","3       0  1564802374  Mon Apr 20 04:58:47 PDT 2009  NO_QUERY    PaterzAttack   \n","4       0  2244269780  Fri Jun 19 14:31:34 PDT 2009  NO_QUERY  jessica_butler   \n","\n","                                           TweetText  \n","0  @calisummer Amen sister!!!  I gotta go left st...  \n","1  my alltel is merging with verizon...booo! I've...  \n","2  Reading my tweets on the UND Graduation, some ...  \n","3  goodnight, 37 followers. early night for me......  \n","4  Fun fact #1 I jumped off a bridge once and hav...  \n"]}]},{"cell_type":"markdown","source":["## Régression Logistique TF-IDF"],"metadata":{"id":"aJM20h_dYPHQ"}},{"cell_type":"code","source":["import os\n","import pandas as pd\n","import joblib\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import classification_report, accuracy_score, f1_score\n","\n","import mlflow\n","import mlflow.sklearn\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# --------------------------------------------------\n","# 1. Configuration MLflow (stockage Google Drive)\n","# --------------------------------------------------\n","mlflow.set_tracking_uri(\"file:/content/drive/MyDrive/mlruns\")\n","mlflow.set_experiment(\"Sentiment_Analysis_Models\")\n","\n","# --------------------------------------------------\n","# 2. Chargement du DataFrame et préparation des données\n","# --------------------------------------------------\n","data_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/train_data.csv'\n","balanced_data = pd.read_csv(data_path)[['target', 'TweetText']]\n","\n","train_data, temp_data = train_test_split(\n","    balanced_data,\n","    test_size=0.3,\n","    random_state=42,\n","    stratify=balanced_data['target']\n",")\n","val_data, test_data = train_test_split(\n","    temp_data,\n","    test_size=0.5,\n","    random_state=42,\n","    stratify=temp_data['target']\n",")\n","\n","# --------------------------------------------------\n","# 3. Vectorisation TF-IDF\n","# --------------------------------------------------\n","max_features = 5000\n","vectorizer = TfidfVectorizer(max_features=max_features)\n","\n","X_train_tfidf = vectorizer.fit_transform(train_data['TweetText'])\n","y_train = train_data['target']\n","\n","X_val_tfidf = vectorizer.transform(val_data['TweetText'])\n","y_val = val_data['target']\n","\n","X_test_tfidf = vectorizer.transform(test_data['TweetText'])\n","y_test = test_data['target']\n","\n","# --------------------------------------------------\n","# 4. GridSearchCV pour la LogReg\n","# --------------------------------------------------\n","param_grid = {\n","    'C': [0.01, 0.1, 1, 10],\n","    'solver': ['lbfgs', 'liblinear'],\n","    'max_iter': [100, 500, 1000]\n","}\n","\n","grid_search = GridSearchCV(\n","    LogisticRegression(random_state=42),\n","    param_grid,\n","    cv=3,\n","    scoring='f1',\n","    verbose=1,\n","    n_jobs=-1\n",")\n","\n","# --------------------------------------------------\n","# 5. Entraînement + tracking MLflow\n","# --------------------------------------------------\n","with mlflow.start_run(run_name=\"TF-IDF_RegressionLogistique\"):\n","    mlflow.log_param(\"model\", \"Logistic Regression\")\n","    mlflow.log_param(\"vectorizer\", \"TF-IDF\")\n","    mlflow.log_param(\"max_features\", max_features)\n","\n","    mlflow.sklearn.autolog()\n","\n","    # GridSearch\n","    grid_search.fit(X_train_tfidf, y_train)\n","    best_model = grid_search.best_estimator_\n","    best_params = grid_search.best_params_\n","\n","    # Log des meilleurs hyperparamètres\n","    mlflow.log_params(best_params)\n","\n","    # Validation\n","    y_val_pred = best_model.predict(X_val_tfidf)\n","    val_accuracy = accuracy_score(y_val, y_val_pred)\n","    val_f1 = f1_score(y_val, y_val_pred)\n","    mlflow.log_metric(\"val_accuracy\", val_accuracy)\n","    mlflow.log_metric(\"val_f1_score\", val_f1)\n","\n","    # Test\n","    y_test_pred = best_model.predict(X_test_tfidf)\n","    test_accuracy = accuracy_score(y_test, y_test_pred)\n","    test_f1 = f1_score(y_test, y_test_pred)\n","    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n","    mlflow.log_metric(\"test_f1_score\", test_f1)\n","\n","    # Rapport de classification\n","    report = classification_report(y_test, y_test_pred)\n","    report_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/classification_report_LogReg.txt'\n","    with open(report_path, 'w') as f:\n","        f.write(report)\n","    mlflow.log_artifact(report_path)\n","\n","    # --------------------------------------------------\n","    # 6. Sauvegarde du modèle et du vectoriseur\n","    # --------------------------------------------------\n","    model_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/tfidf_logistic_regression_best.pkl'\n","    vect_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/tfidf_vectorizer.pkl'\n","    joblib.dump(best_model, model_path)\n","    joblib.dump(vectorizer, vect_path)\n","    mlflow.log_artifact(model_path)\n","    mlflow.log_artifact(vect_path)\n","\n","    print(\"\\nValidation Accuracy :\", val_accuracy)\n","    print(\"Validation F1 Score :\", val_f1)\n","    print(\"\\nTest Accuracy :\", test_accuracy)\n","    print(\"Test F1 Score :\", test_f1)\n","    print(\"\\nClassification Report (Test):\\n\", report)\n","\n","print(\"[INFO] Entraînement et évaluation terminés. Les logs sont dans 'file:/content/drive/MyDrive/mlruns'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mEbdvGbtgh8G","executionInfo":{"status":"ok","timestamp":1736098453093,"user_tz":-60,"elapsed":28961,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"179acdc1-b183-4c34-c592-d9b8fe35fa84"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["2025/01/05 17:33:50 WARNING mlflow.utils.autologging_utils: MLflow sklearn autologging is known to be compatible with 0.24.1 <= scikit-learn <= 1.5.2, but the installed version is 1.6.0. If you encounter errors during autologging, try upgrading / downgrading scikit-learn to a compatible version, or try upgrading MLflow.\n","2025/01/05 17:33:54 WARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'Series' object has no attribute 'toarray'\n"]},{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"]},{"output_type":"stream","name":"stderr","text":["2025/01/05 17:34:12 INFO mlflow.sklearn.utils: Logging the 5 best runs, 19 runs will be omitted.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Validation Accuracy : 0.7805333333333333\n","Validation F1 Score : 0.783776683087028\n","\n","Test Accuracy : 0.7827\n","Test F1 Score : 0.7844174741228215\n","\n","Classification Report (Test):\n","               precision    recall  f1-score   support\n","\n","           0       0.79      0.77      0.78     15000\n","           1       0.78      0.79      0.78     15000\n","\n","    accuracy                           0.78     30000\n","   macro avg       0.78      0.78      0.78     30000\n","weighted avg       0.78      0.78      0.78     30000\n","\n","[INFO] Entraînement et évaluation terminés. Les logs sont dans 'file:/content/drive/MyDrive/mlruns'.\n"]}]},{"cell_type":"markdown","source":["**Conclusion sur le modèle baseline : Régression Logistique avec TF-IDF**\n","\n","Le modèle baseline utilisant une régression logistique avec des vecteurs TF-IDF obtient des résultats cohérents sur les ensembles de validation et de test :\n","\n","- **Accuracy :** 78% sur les deux ensembles, montrant une bonne capacité de généralisation.\n","- **F1-Score :** 78%, indiquant un bon équilibre entre précision et rappel pour les deux classes.\n","- Les scores pour les classes `0` (négatif) et `1` (positif) sont équilibrés, sans biais vers l'une des classes.\n","\n","Points forts\n","- Modèle simple et rapide à entraîner.\n","- Bonne généralisation des performances entre validation et test.\n","\n","Limites\n","- Ne capture pas les relations contextuelles profondes entre les mots, ce qui limite sa capacité sur des données plus complexes.\n","\n","Perspectives\n","Cette baseline offre un point de comparaison pour des modèles avancés comme Word2Vec, FastText, ou des architectures neuronales (LSTM, GRU, BERT).\n"],"metadata":{"id":"ObE-FDrsXN4a"}},{"cell_type":"markdown","source":["## Word2Vec_LSTM"],"metadata":{"id":"kvUyVSEWYczC"}},{"cell_type":"code","source":["import os\n","import pickle\n","import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import mlflow\n","import mlflow.tensorflow\n","import gensim.downloader as api\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, f1_score\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import (\n","    Bidirectional, Dense, Dropout, Embedding, LSTM\n",")\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# --------------------------------------------------\n","# 1. Vérification GPU & Activation XLA\n","# --------------------------------------------------\n","device_name = tf.test.gpu_device_name()\n","if device_name == '/device:GPU:0':\n","    print(f\"[INFO] GPU détecté: {device_name}\")\n","    tf.config.optimizer.set_jit(True)  # Active XLA pour accélérer un peu\n","else:\n","    print(\"[WARNING] Aucun GPU détecté, exécution sur CPU.\")\n","\n","# --------------------------------------------------\n","# 2. Configuration MLflow (stockage Google Drive)\n","# --------------------------------------------------\n","mlflow.set_tracking_uri(\"file:/content/drive/MyDrive/mlruns\")\n","mlflow.set_experiment(\"Sentiment_Analysis_Models\")\n","\n","# --------------------------------------------------\n","# 3. Chargement du DataFrame\n","# --------------------------------------------------\n","data_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/train_data.csv'\n","balanced_data = pd.read_csv(data_path)[['target', 'TweetText']]\n","\n","# --------------------------------------------------\n","# 4. Division train/val/test\n","# --------------------------------------------------\n","train_data, temp_data = train_test_split(\n","    balanced_data,\n","    test_size=0.3,\n","    random_state=42,\n","    stratify=balanced_data['target']\n",")\n","val_data, test_data = train_test_split(\n","    temp_data,\n","    test_size=0.5,\n","    random_state=42,\n","    stratify=temp_data['target']\n",")\n","\n","# --------------------------------------------------\n","# 5. Tokenisation et préparation des séquences\n","# --------------------------------------------------\n","max_features = 10_000\n","max_len = 50\n","\n","tokenizer = Tokenizer(num_words=max_features)\n","tokenizer.fit_on_texts(train_data['TweetText'])\n","\n","def preprocess_texts(texts):\n","    seqs = tokenizer.texts_to_sequences(texts)\n","    return pad_sequences(seqs, maxlen=max_len)\n","\n","X_train = preprocess_texts(train_data['TweetText'])\n","X_val = preprocess_texts(val_data['TweetText'])\n","X_test = preprocess_texts(test_data['TweetText'])\n","\n","y_train = train_data['target'].values\n","y_val = val_data['target'].values\n","y_test = test_data['target'].values\n","\n","# --------------------------------------------------\n","# 6. Chargement des embeddings GloVe (glove-twitter-50)\n","#    => plus légers, dimension = 50\n","# --------------------------------------------------\n","embedding_dim = 50\n","word_vectors = api.load('glove-twitter-50')\n","print(\"[INFO] Embeddings GloVe (50-dim) chargés avec succès.\")\n","\n","word_index = tokenizer.word_index\n","num_words = min(len(word_index) + 1, max_features)\n","\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","    if word in word_vectors:\n","        embedding_matrix[i] = word_vectors[word]\n","    else:\n","        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n","\n","# --------------------------------------------------\n","# 7. Construction d'un modèle LSTM plus léger\n","#    - Une seule couche LSTM, 32 unités\n","#    - Bidirectionnel\n","# --------------------------------------------------\n","model = Sequential([\n","    Embedding(\n","        input_dim=num_words,\n","        output_dim=embedding_dim,\n","        weights=[embedding_matrix],\n","        input_shape=(max_len,),\n","        trainable=False\n","    ),\n","    Bidirectional(LSTM(32, dropout=0.2, recurrent_dropout=0.2)),\n","    Dense(16, activation='relu'),\n","    Dropout(0.5),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# --------------------------------------------------\n","# 8. Définition des callbacks\n","# --------------------------------------------------\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=2,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","checkpoint_callback = ModelCheckpoint(\n","    filepath='model_checkpoint_lstm_light.keras',\n","    save_best_only=True,\n","    monitor='val_loss',\n","    mode='min',\n","    save_weights_only=False,\n","    verbose=1\n",")\n","\n","# --------------------------------------------------\n","# 9. Entraînement + Tracking MLflow\n","# --------------------------------------------------\n","with mlflow.start_run(run_name='Light_GloVe_LSTM', nested=True):\n","    mlflow.log_param('model', 'Light_Bidirectional_LSTM')\n","    mlflow.log_param('embedding_dim', embedding_dim)\n","    mlflow.log_param('max_features', max_features)\n","    mlflow.log_param('max_len', max_len)\n","\n","    # Sauvegarder le tokenizer\n","    with open('tokenizer.pkl', 'wb') as f:\n","        pickle.dump(tokenizer, f)\n","    mlflow.log_artifact('tokenizer.pkl')\n","\n","    start_time = time.time()\n","    history = model.fit(\n","        X_train, y_train,\n","        validation_data=(X_val, y_val),\n","        epochs=10,\n","        batch_size=64,\n","        verbose=1,\n","        callbacks=[early_stopping, checkpoint_callback]\n","    )\n","    end_time = time.time()\n","\n","    # Temps d'entraînement\n","    training_time = end_time - start_time\n","    mlflow.log_metric('training_time', training_time)\n","\n","    # Évaluation\n","    y_pred_prob = model.predict(X_test)\n","    y_pred = (y_pred_prob > 0.5).astype('int32')\n","\n","    test_accuracy = accuracy_score(y_test, y_pred)\n","    test_f1 = f1_score(y_test, y_pred)\n","\n","    mlflow.log_metric('test_accuracy', test_accuracy)\n","    mlflow.log_metric('test_f1_score', test_f1)\n","\n","    # Rapport de classification\n","    report = classification_report(y_test, y_pred)\n","    mlflow.log_text(report, 'classification_report_Light_LSTM.txt')\n","\n","    # Sauvegarde du modèle final\n","    model.save('final_trained_light_LSTM.h5')\n","    mlflow.log_artifact('final_trained_light_LSTM.h5')\n","\n","print(\"[INFO] Entraînement terminé. Logs enregistrés sur 'file:/content/drive/MyDrive/mlruns'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vi1B4M0698uT","executionInfo":{"status":"ok","timestamp":1736097772572,"user_tz":-60,"elapsed":690963,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"9a5b1035-eb93-4ae4-dda4-ee4368e4285d"},"execution_count":3,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["[INFO] GPU détecté: /device:GPU:0\n","[==================================================] 100.0% 199.5/199.5MB downloaded\n","[INFO] Embeddings GloVe (50-dim) chargés avec succès.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.6545 - loss: 0.6158\n","Epoch 1: val_loss improved from inf to 0.52248, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 86ms/step - accuracy: 0.6545 - loss: 0.6158 - val_accuracy: 0.7387 - val_loss: 0.5225\n","Epoch 2/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7217 - loss: 0.5522\n","Epoch 2: val_loss improved from 0.52248 to 0.49484, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 86ms/step - accuracy: 0.7217 - loss: 0.5522 - val_accuracy: 0.7566 - val_loss: 0.4948\n","Epoch 3/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7385 - loss: 0.5319\n","Epoch 3: val_loss improved from 0.49484 to 0.48213, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 85ms/step - accuracy: 0.7385 - loss: 0.5319 - val_accuracy: 0.7648 - val_loss: 0.4821\n","Epoch 4/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7494 - loss: 0.5166\n","Epoch 4: val_loss improved from 0.48213 to 0.47605, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 85ms/step - accuracy: 0.7494 - loss: 0.5166 - val_accuracy: 0.7706 - val_loss: 0.4761\n","Epoch 5/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7531 - loss: 0.5120\n","Epoch 5: val_loss improved from 0.47605 to 0.46995, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 85ms/step - accuracy: 0.7531 - loss: 0.5120 - val_accuracy: 0.7747 - val_loss: 0.4700\n","Epoch 6/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7581 - loss: 0.5072\n","Epoch 6: val_loss improved from 0.46995 to 0.46525, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 84ms/step - accuracy: 0.7581 - loss: 0.5072 - val_accuracy: 0.7764 - val_loss: 0.4653\n","Epoch 7/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7633 - loss: 0.5003\n","Epoch 7: val_loss improved from 0.46525 to 0.46291, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 85ms/step - accuracy: 0.7633 - loss: 0.5003 - val_accuracy: 0.7758 - val_loss: 0.4629\n","Epoch 8/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7625 - loss: 0.4988\n","Epoch 8: val_loss improved from 0.46291 to 0.45830, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 84ms/step - accuracy: 0.7625 - loss: 0.4988 - val_accuracy: 0.7803 - val_loss: 0.4583\n","Epoch 9/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.7666 - loss: 0.4916\n","Epoch 9: val_loss improved from 0.45830 to 0.45805, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 84ms/step - accuracy: 0.7666 - loss: 0.4916 - val_accuracy: 0.7799 - val_loss: 0.4580\n","Epoch 10/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.7682 - loss: 0.4901\n","Epoch 10: val_loss improved from 0.45805 to 0.45643, saving model to model_checkpoint_lstm_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 86ms/step - accuracy: 0.7682 - loss: 0.4901 - val_accuracy: 0.7803 - val_loss: 0.4564\n","Restoring model weights from the end of the best epoch: 10.\n","\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 21ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Entraînement terminé. Logs enregistrés sur 'file:/content/drive/MyDrive/mlruns'.\n"]}]},{"cell_type":"markdown","source":["**Conclusion sur le modèle Word2Vec_LSTM**  \n","Résultats du modèle Word2Vec avec LSTM\n","\n","\n","Points forts\n","- Amélioration par rapport au baseline, démontrant la pertinence des embeddings Word2Vec et des LSTM pour capturer la structure des données textuelles.\n","- Bonne performance sur les deux classes sans déséquilibre significatif.\n","\n","Limites\n","- Entraînement relativement long (94s par époque).\n","- Peut surperformer légèrement sur des données équilibrées, mais pourrait nécessiter une évaluation sur des données déséquilibrées pour confirmer sa robustesse.\n","\n","Perspectives\n","Ce modèle constitue une avancée par rapport au baseline. Des essais avec des architectures plus complexes (e.g., GRU, BERT) pourraient encore améliorer les performances, notamment pour des données plus complexes ou déséquilibrées.\n","\n","\n","Pourquoi utiliser un GRU ?\n","\n","- **Efficacité** : Plus rapide et moins coûteux en mémoire que LSTM.\n","- **Performances similaires** : Suffisant pour l'analyse de sentiments avec des séquences courtes comme les tweets.\n","- **Optimisation** : Capture efficacement les relations séquentielles avec moins de paramètres.\n","\n","---\n","\n","Ajustements réalisés\n","\n","1. **GRU au lieu de LSTM** : Réduction des paramètres pour accélérer l'entraînement.\n","2. **Embeddings Word2Vec réduits** : Taille des vecteurs passée de 300 à **100 dimensions**.\n","3. **Longueur des séquences ajustée** : `max_len` réduit à **30 tokens** pour correspondre aux tweets.\n","4. **Optimiseur AdamW** : Appliqué avec un **learning rate** de `1e-4`.\n","5. **Taille de batch ajustée** : Réduite à **64** pour un meilleur compromis entre rapidité et mémoire.\n"],"metadata":{"id":"sgIdG954YfwG"}},{"cell_type":"markdown","source":["## Word2Vec avec GRU"],"metadata":{"id":"Pm4szP_BaxTF"}},{"cell_type":"code","source":["##########################################################\n","# Code optimisé pour la classification de sentiments\n","# via embeddings GloVe + GRU bidirectionnel, suivi MLflow\n","##########################################################\n","\n","import os\n","import pickle\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, classification_report, f1_score\n","from sklearn.model_selection import train_test_split\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import gensim.downloader as api\n","import mlflow\n","import mlflow.tensorflow\n","\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import (Bidirectional, Dense, Dropout, Embedding,\n","                                     GRU)\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# --------------------------------------------------\n","# 1. Vérifier la dispo du GPU et activer XLA (optionnel)\n","# --------------------------------------------------\n","device_name = tf.test.gpu_device_name()\n","if device_name == '/device:GPU:0':\n","    print(\"GPU détecté :\", device_name)\n","    # Optionnel : activer XLA pour booster les perfs\n","    tf.config.optimizer.set_jit(True)  # Active l'optimisation XLA\n","else:\n","    print(\"Aucun GPU détecté, exécution sur CPU.\")\n","\n","# --------------------------------------------------\n","# 2. Configuration de MLflow : stockage dans Google Drive\n","# --------------------------------------------------\n","mlflow.set_tracking_uri(\"file:/content/drive/MyDrive/mlruns\")\n","mlflow.set_experiment(\"Sentiment_Analysis_Models\")\n","\n","# --------------------------------------------------\n","# 3. Chargement du DataFrame\n","# --------------------------------------------------\n","data_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/train_data.csv'\n","balanced_data = pd.read_csv(data_path)\n","balanced_data = balanced_data[['target', 'TweetText']]\n","\n","# --------------------------------------------------\n","# 4. Préparation des données : train / validation / test\n","# --------------------------------------------------\n","train_data, temp_data = train_test_split(\n","    balanced_data,\n","    test_size=0.3,\n","    random_state=42,\n","    stratify=balanced_data['target']\n",")\n","val_data, test_data = train_test_split(\n","    temp_data,\n","    test_size=0.5,\n","    random_state=42,\n","    stratify=temp_data['target']\n",")\n","\n","# --------------------------------------------------\n","# 5. Paramètres de tokenisation et prétraitement\n","# --------------------------------------------------\n","max_features = 10000  # Nombre max de mots à considérer\n","max_len = 50          # Longueur max des séquences\n","\n","tokenizer = Tokenizer(num_words=max_features)\n","tokenizer.fit_on_texts(train_data['TweetText'])\n","\n","def preprocess_texts(texts):\n","    sequences = tokenizer.texts_to_sequences(texts)\n","    return pad_sequences(sequences, maxlen=max_len)\n","\n","X_train = preprocess_texts(train_data['TweetText'])\n","X_val = preprocess_texts(val_data['TweetText'])\n","X_test = preprocess_texts(test_data['TweetText'])\n","\n","y_train = train_data['target'].values\n","y_val = val_data['target'].values\n","y_test = test_data['target'].values\n","\n","# --------------------------------------------------\n","# 6. Chargement des embeddings GloVe\n","# --------------------------------------------------\n","embedding_dim = 100\n","print(\"Chargement des embeddings GloVe...\")\n","word_vectors = api.load('glove-twitter-100')  # Embeddings légers\n","print(\"Embeddings GloVe chargés avec succès.\")\n","\n","# --------------------------------------------------\n","# 7. Construction de la matrice d'embeddings\n","# --------------------------------------------------\n","word_index = tokenizer.word_index\n","num_words = min(len(word_index) + 1, max_features)\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","    if word in word_vectors:\n","        embedding_matrix[i] = word_vectors[word]\n","    else:\n","        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n","\n","# --------------------------------------------------\n","# 8. Construction du modèle GRU Bidirectionnel\n","# --------------------------------------------------\n","model = Sequential([\n","    Embedding(\n","        input_dim=num_words,\n","        output_dim=embedding_dim,\n","        weights=[embedding_matrix],\n","        input_shape=(max_len,),\n","        trainable=False  # Mettre True si on veut affiner l'embedding\n","    ),\n","    Bidirectional(GRU(64, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)),\n","    Bidirectional(GRU(32, dropout=0.2, recurrent_dropout=0.2)),\n","    Dense(32, activation='relu'),\n","    Dropout(0.5),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","print(\"Résumé du modèle :\")\n","model.summary()\n","\n","# --------------------------------------------------\n","# 9. Callbacks Keras (EarlyStopping, Checkpoint)\n","# --------------------------------------------------\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=2,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","checkpoint_callback = ModelCheckpoint(\n","    filepath='model_checkpoint.keras',\n","    save_best_only=True,\n","    monitor='val_loss',\n","    mode='min',\n","    save_weights_only=False,  # on veut sauvegarder la structure+poids\n","    verbose=1\n",")\n","\n","# --------------------------------------------------\n","# 10. Entraînement + tracking MLflow\n","# --------------------------------------------------\n","with mlflow.start_run(run_name='Optimized_Word2Vec_GRU', nested=True):\n","    mlflow.log_param('model', 'Bidirectional_GRU')\n","    mlflow.log_param('embedding_dim', embedding_dim)\n","    mlflow.log_param('max_features', max_features)\n","    mlflow.log_param('max_len', max_len)\n","\n","    # Sauvegarder le tokenizer comme artefact\n","    with open('tokenizer.pkl', 'wb') as f:\n","        pickle.dump(tokenizer, f)\n","    mlflow.log_artifact('tokenizer.pkl')\n","\n","    start_time = time.time()\n","    history = model.fit(\n","        X_train, y_train,\n","        validation_data=(X_val, y_val),\n","        epochs=10,\n","        batch_size=64,  # batch size possible à ajuster selon la RAM GPU\n","        verbose=1,\n","        callbacks=[early_stopping, checkpoint_callback]\n","    )\n","    end_time = time.time()\n","\n","    # Logguer le temps d'entraînement\n","    training_time = end_time - start_time\n","    mlflow.log_metric('training_time', training_time)\n","\n","    # --------------------------------------------------\n","    # Évaluation\n","    # --------------------------------------------------\n","    y_pred_prob = model.predict(X_test)\n","    y_pred = (y_pred_prob > 0.5).astype('int32')\n","    test_accuracy = accuracy_score(y_test, y_pred)\n","    test_f1 = f1_score(y_test, y_pred)\n","\n","    mlflow.log_metric('test_accuracy', test_accuracy)\n","    mlflow.log_metric('test_f1_score', test_f1)\n","\n","    # Enregistrer le rapport de classification dans MLflow\n","    report = classification_report(y_test, y_pred)\n","    mlflow.log_text(report, 'classification_report_Optimized_GRU.txt')\n","    print(\"\\nClassification Report:\\n\", report)\n","\n","    # --------------------------------------------------\n","    # Sauvegarde finale du meilleur modèle pour déploiement\n","    # --------------------------------------------------\n","    # ModelCheckpoint a déjà sauvegardé le meilleur modèle,\n","    # mais on peut aussi sauvegarder la version finale\n","    model.save('final_trained_model.h5')\n","    mlflow.log_artifact('final_trained_model.h5')\n","\n","print(\"Entraînement et évaluation terminés. Les logs sont dans 'file:/content/drive/MyDrive/mlruns'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"tneJiWrmUjOd","executionInfo":{"status":"ok","timestamp":1736091958571,"user_tz":-60,"elapsed":5489618,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"76b73205-5115-4179-88cb-10c0fdd10e66"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","GPU détecté : /device:GPU:0\n","Chargement des embeddings GloVe...\n","Embeddings GloVe chargés avec succès.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Résumé du modèle :\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_4\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_4 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m100\u001b[0m)             │       \u001b[38;5;34m1,000,000\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ bidirectional_8 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m63,744\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ bidirectional_9 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m31,104\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m2,080\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ embedding_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ bidirectional_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">63,744</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ bidirectional_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">31,104</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,096,961\u001b[0m (4.18 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,096,961</span> (4.18 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m96,961\u001b[0m (378.75 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96,961</span> (378.75 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,000,000\u001b[0m (3.81 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> (3.81 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 232ms/step - accuracy: 0.6107 - loss: 0.9706\n","Epoch 1: val_loss improved from inf to 0.56519, saving model to model_checkpoint.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 242ms/step - accuracy: 0.6107 - loss: 0.9705 - val_accuracy: 0.7079 - val_loss: 0.5652\n","Epoch 2/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.7057 - loss: 0.5775\n","Epoch 2: val_loss improved from 0.56519 to 0.53816, saving model to model_checkpoint.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m531s\u001b[0m 243ms/step - accuracy: 0.7057 - loss: 0.5775 - val_accuracy: 0.7313 - val_loss: 0.5382\n","Epoch 3/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.7309 - loss: 0.5448\n","Epoch 3: val_loss improved from 0.53816 to 0.52824, saving model to model_checkpoint.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 242ms/step - accuracy: 0.7309 - loss: 0.5448 - val_accuracy: 0.7431 - val_loss: 0.5282\n","Epoch 4/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.7456 - loss: 0.5218\n","Epoch 4: val_loss improved from 0.52824 to 0.52093, saving model to model_checkpoint.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 240ms/step - accuracy: 0.7456 - loss: 0.5218 - val_accuracy: 0.7489 - val_loss: 0.5209\n","Epoch 5/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step - accuracy: 0.7554 - loss: 0.5100\n","Epoch 5: val_loss improved from 0.52093 to 0.51173, saving model to model_checkpoint.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 242ms/step - accuracy: 0.7554 - loss: 0.5100 - val_accuracy: 0.7452 - val_loss: 0.5117\n","Epoch 6/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.7501 - loss: 0.5387\n","Epoch 6: val_loss improved from 0.51173 to 0.51002, saving model to model_checkpoint.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m534s\u001b[0m 244ms/step - accuracy: 0.7501 - loss: 0.5387 - val_accuracy: 0.7523 - val_loss: 0.5100\n","Epoch 7/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 235ms/step - accuracy: 0.7590 - loss: 0.5054\n","Epoch 7: val_loss improved from 0.51002 to 0.50250, saving model to model_checkpoint.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m536s\u001b[0m 245ms/step - accuracy: 0.7590 - loss: 0.5054 - val_accuracy: 0.7582 - val_loss: 0.5025\n","Epoch 8/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step - accuracy: 0.7647 - loss: 0.4929\n","Epoch 8: val_loss did not improve from 0.50250\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m537s\u001b[0m 245ms/step - accuracy: 0.7647 - loss: 0.4929 - val_accuracy: 0.7582 - val_loss: 0.5033\n","Epoch 9/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 238ms/step - accuracy: 0.7645 - loss: 0.4938\n","Epoch 9: val_loss improved from 0.50250 to 0.49167, saving model to model_checkpoint.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 248ms/step - accuracy: 0.7645 - loss: 0.4938 - val_accuracy: 0.7635 - val_loss: 0.4917\n","Epoch 10/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 239ms/step - accuracy: 0.7741 - loss: 0.4842\n","Epoch 10: val_loss did not improve from 0.49167\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 248ms/step - accuracy: 0.7741 - loss: 0.4842 - val_accuracy: 0.7645 - val_loss: 0.4943\n","Restoring model weights from the end of the best epoch: 9.\n","\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 46ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNI [absl] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.74      0.82      0.78     15000\n","           1       0.80      0.71      0.75     15000\n","\n","    accuracy                           0.76     30000\n","   macro avg       0.77      0.76      0.76     30000\n","weighted avg       0.77      0.76      0.76     30000\n","\n","Entraînement et évaluation terminés. Les logs sont dans 'file:/content/drive/MyDrive/mlruns'.\n"]}]},{"cell_type":"markdown","source":["## GLOVE GRU"],"metadata":{"id":"8F-NANIeYl68"}},{"cell_type":"code","source":["import os\n","import pickle\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import mlflow\n","import mlflow.tensorflow\n","import gensim.downloader as api\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report, f1_score\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras.layers import Bidirectional, Dense, Dropout, Embedding, GRU\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# --------------------------------------------------\n","# 1. Vérification GPU & Activation XLA (optionnel)\n","# --------------------------------------------------\n","device_name = tf.test.gpu_device_name()\n","if device_name == '/device:GPU:0':\n","    print(f\"[INFO] GPU détecté: {device_name}\")\n","    tf.config.optimizer.set_jit(True)  # Active XLA (peut accélérer un peu)\n","else:\n","    print(\"[WARNING] Aucun GPU détecté, exécution sur CPU.\")\n","\n","# --------------------------------------------------\n","# 2. Configuration MLflow (Google Drive)\n","# --------------------------------------------------\n","mlflow.set_tracking_uri(\"file:/content/drive/MyDrive/mlruns\")\n","mlflow.set_experiment(\"Sentiment_Analysis_Models\")\n","\n","# --------------------------------------------------\n","# 3. Chargement du DataFrame\n","#     Ex.: 200k tweets (100k neg, 100k pos)\n","# --------------------------------------------------\n","data_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/train_data.csv'\n","balanced_data = pd.read_csv(data_path)[['target', 'TweetText']]\n","\n","# --------------------------------------------------\n","# 4. Division train/val/test (70%/15%/15%)\n","# --------------------------------------------------\n","train_data, temp_data = train_test_split(\n","    balanced_data,\n","    test_size=0.3,\n","    random_state=42,\n","    stratify=balanced_data['target']\n",")\n","val_data, test_data = train_test_split(\n","    temp_data,\n","    test_size=0.5,\n","    random_state=42,\n","    stratify=temp_data['target']\n",")\n","\n","# --------------------------------------------------\n","# 5. Tokenisation et préparation des séquences\n","# --------------------------------------------------\n","max_features = 10_000  # Ajuste si besoin (vocab)\n","max_len = 50\n","\n","tokenizer = Tokenizer(num_words=max_features)\n","tokenizer.fit_on_texts(train_data['TweetText'])\n","\n","def preprocess_texts(texts):\n","    seqs = tokenizer.texts_to_sequences(texts)\n","    return pad_sequences(seqs, maxlen=max_len)\n","\n","X_train = preprocess_texts(train_data['TweetText'])\n","X_val = preprocess_texts(val_data['TweetText'])\n","X_test = preprocess_texts(test_data['TweetText'])\n","\n","y_train = train_data['target'].values\n","y_val = val_data['target'].values\n","y_test = test_data['target'].values\n","\n","# --------------------------------------------------\n","# 6. Chargement embeddings GloVe (twitter-50)\n","#    -> 50 dimensions (plus léger)\n","# --------------------------------------------------\n","embedding_dim = 50\n","word_vectors = api.load('glove-twitter-50')\n","print(\"[INFO] Embeddings GloVe (50-dim) chargés avec succès.\")\n","\n","word_index = tokenizer.word_index\n","num_words = min(len(word_index) + 1, max_features)\n","\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","    if word in word_vectors:\n","        embedding_matrix[i] = word_vectors[word]\n","    else:\n","        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n","\n","# --------------------------------------------------\n","# 7. Modèle GRU allégé\n","#    - Une seule couche GRU (64)\n","#    - Dense réduit (32)\n","# --------------------------------------------------\n","model = Sequential([\n","    Embedding(\n","        input_dim=num_words,\n","        output_dim=embedding_dim,\n","        weights=[embedding_matrix],\n","        input_shape=(max_len,),\n","        trainable=False  # Ne pas affiner l'embedding, + léger\n","    ),\n","    Bidirectional(GRU(64, dropout=0.2, recurrent_dropout=0.2)),\n","    Dense(32, activation='relu'),\n","    Dropout(0.5),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# --------------------------------------------------\n","# 8. Callbacks\n","# --------------------------------------------------\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    patience=2,\n","    restore_best_weights=True,\n","    verbose=1\n",")\n","\n","checkpoint_callback = ModelCheckpoint(\n","    filepath='model_checkpoint_light.keras',\n","    save_best_only=True,\n","    monitor='val_loss',\n","    mode='min',\n","    save_weights_only=False,\n","    verbose=1\n",")\n","\n","# --------------------------------------------------\n","# 9. Entraînement + Tracking MLflow\n","# --------------------------------------------------\n","with mlflow.start_run(run_name='Light_GloVe_GRU', nested=True):\n","    mlflow.log_param('model', 'Bidirectional_GRU_light')\n","    mlflow.log_param('embedding', 'glove-twitter-50')\n","    mlflow.log_param('embedding_dim', embedding_dim)\n","    mlflow.log_param('max_features', max_features)\n","    mlflow.log_param('max_len', max_len)\n","\n","    # Sauvegarde du tokenizer\n","    with open('tokenizer.pkl', 'wb') as f:\n","        pickle.dump(tokenizer, f)\n","    mlflow.log_artifact('tokenizer.pkl')\n","\n","    print(\"[INFO] Début de l'entraînement ...\")\n","    start_time = time.time()\n","    history = model.fit(\n","        X_train, y_train,\n","        validation_data=(X_val, y_val),\n","        epochs=10,\n","        batch_size=64,  # 128 possible si tu as plus de mémoire\n","        verbose=1,\n","        callbacks=[early_stopping, checkpoint_callback]\n","    )\n","    end_time = time.time()\n","\n","    training_time = end_time - start_time\n","    mlflow.log_metric('training_time', training_time)\n","\n","    print(\"[INFO] Évaluation sur l'ensemble de test ...\")\n","    y_pred_prob = model.predict(X_test)\n","    y_pred = (y_pred_prob > 0.5).astype('int32')\n","\n","    test_accuracy = accuracy_score(y_test, y_pred)\n","    test_f1 = f1_score(y_test, y_pred)\n","\n","    mlflow.log_metric('test_accuracy', test_accuracy)\n","    mlflow.log_metric('test_f1_score', test_f1)\n","\n","    report = classification_report(y_test, y_pred)\n","    mlflow.log_text(report, 'classification_report_Light_GloVe_GRU.txt')\n","\n","    # Sauvegarde du modèle final\n","    model.save('final_light_GloVe_GRU.h5')\n","    mlflow.log_artifact('final_light_GloVe_GRU.h5')\n","\n","    print(f\"[INFO] Test Accuracy: {test_accuracy:.4f}, Test F1: {test_f1:.4f}\")\n","    print(\"[INFO] Rapport de classification sauvegardé dans MLflow.\")\n","\n","print(\"[INFO] Modèle entraîné. Logs dans 'file:/content/drive/MyDrive/mlruns'.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tT_6tNkyJth","executionInfo":{"status":"ok","timestamp":1736094960351,"user_tz":-60,"elapsed":2895492,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"869e91e5-5e96-4a75-a327-d5906df5fedc","collapsed":true},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] GPU détecté: /device:GPU:0\n","[==================================================] 100.0% 199.5/199.5MB downloaded\n","[INFO] Embeddings GloVe (50-dim) chargés avec succès.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(**kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Début de l'entraînement ...\n","Epoch 1/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.6645 - loss: 0.6418\n","Epoch 1: val_loss improved from inf to 0.56314, saving model to model_checkpoint_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m289s\u001b[0m 129ms/step - accuracy: 0.6645 - loss: 0.6417 - val_accuracy: 0.7155 - val_loss: 0.5631\n","Epoch 2/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.7148 - loss: 0.5581\n","Epoch 2: val_loss improved from 0.56314 to 0.54937, saving model to model_checkpoint_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 128ms/step - accuracy: 0.7148 - loss: 0.5581 - val_accuracy: 0.7281 - val_loss: 0.5494\n","Epoch 3/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7283 - loss: 0.5401\n","Epoch 3: val_loss improved from 0.54937 to 0.54040, saving model to model_checkpoint_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 126ms/step - accuracy: 0.7284 - loss: 0.5401 - val_accuracy: 0.7338 - val_loss: 0.5404\n","Epoch 4/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7398 - loss: 0.5261\n","Epoch 4: val_loss improved from 0.54040 to 0.52531, saving model to model_checkpoint_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 127ms/step - accuracy: 0.7398 - loss: 0.5261 - val_accuracy: 0.7452 - val_loss: 0.5253\n","Epoch 5/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7460 - loss: 0.5178\n","Epoch 5: val_loss improved from 0.52531 to 0.51717, saving model to model_checkpoint_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 126ms/step - accuracy: 0.7460 - loss: 0.5178 - val_accuracy: 0.7526 - val_loss: 0.5172\n","Epoch 6/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7521 - loss: 0.5095\n","Epoch 6: val_loss improved from 0.51717 to 0.51414, saving model to model_checkpoint_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 126ms/step - accuracy: 0.7521 - loss: 0.5095 - val_accuracy: 0.7516 - val_loss: 0.5141\n","Epoch 7/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7534 - loss: 0.5060\n","Epoch 7: val_loss did not improve from 0.51414\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m277s\u001b[0m 126ms/step - accuracy: 0.7534 - loss: 0.5060 - val_accuracy: 0.7520 - val_loss: 0.5153\n","Epoch 8/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7577 - loss: 0.5027\n","Epoch 8: val_loss improved from 0.51414 to 0.50797, saving model to model_checkpoint_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 126ms/step - accuracy: 0.7577 - loss: 0.5027 - val_accuracy: 0.7573 - val_loss: 0.5080\n","Epoch 9/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7554 - loss: 0.5049\n","Epoch 9: val_loss improved from 0.50797 to 0.49798, saving model to model_checkpoint_light.keras\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 125ms/step - accuracy: 0.7554 - loss: 0.5049 - val_accuracy: 0.7601 - val_loss: 0.4980\n","Epoch 10/10\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.7595 - loss: 0.4986\n","Epoch 10: val_loss did not improve from 0.49798\n","\u001b[1m2188/2188\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 126ms/step - accuracy: 0.7595 - loss: 0.4986 - val_accuracy: 0.7590 - val_loss: 0.5051\n","Restoring model weights from the end of the best epoch: 9.\n","[INFO] Évaluation sur l'ensemble de test ...\n","\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 23ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNI [absl] You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]},{"output_type":"stream","name":"stdout","text":["[INFO] Test Accuracy: 0.7616, Test F1: 0.7592\n","[INFO] Rapport de classification sauvegardé dans MLflow.\n","[INFO] Modèle entraîné. Logs dans 'file:/content/drive/MyDrive/mlruns'.\n"]}]},{"cell_type":"markdown","source":["## BERT CNN"],"metadata":{"id":"geQ8DC1eDUI_"}},{"cell_type":"code","source":["import torch"],"metadata":{"id":"wvejVInuQtNj","executionInfo":{"status":"ok","timestamp":1736064813612,"user_tz":-60,"elapsed":583,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Pu7vlmmQq0S","executionInfo":{"status":"ok","timestamp":1736064816968,"user_tz":-60,"elapsed":852,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"ac60e447-526a-468e-b1cd-b2e71cc6a999"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}]},{"cell_type":"code","source":["import os\n","import time\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, GlobalMaxPooling1D\n","from tensorflow.keras.models import Model\n","from transformers import BertTokenizer, TFBertModel\n","import mlflow\n","import mlflow.tensorflow\n","from azureml.core import Workspace\n","import tensorflow as tf\n","\n","# Charger le Workspace Azure ML\n","ws = Workspace.from_config(path='/content/drive/MyDrive/config.json')\n","mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n","mlflow.set_experiment('Sentiment_Analysis_Models')\n","\n","# Activer l'autologging\n","mlflow.tensorflow.autolog()\n","\n","# Charger le DataFrame\n","data_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/train_data.csv'\n","balanced_data = pd.read_csv(data_path)\n","balanced_data = balanced_data[['target', 'TweetText']]\n","\n","# Diviser les données en ensembles d'entraînement, validation et test\n","train_data, temp_data = train_test_split(\n","    balanced_data, test_size=0.3, random_state=42, stratify=balanced_data['target']\n",")\n","val_data, test_data = train_test_split(\n","    temp_data, test_size=0.5, random_state=42, stratify=temp_data['target']\n",")\n","\n","# Préparer le tokenizer BERT\n","max_len = 50\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def preprocess_texts_with_bert(texts):\n","    tokens = tokenizer(\n","        list(texts),\n","        max_length=max_len,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=\"tf\"\n","    )\n","    return tokens['input_ids'], tokens['attention_mask']\n","\n","# Prétraitement des données\n","X_train_ids, X_train_mask = preprocess_texts_with_bert(train_data['TweetText'])\n","X_val_ids, X_val_mask = preprocess_texts_with_bert(val_data['TweetText'])\n","X_test_ids, X_test_mask = preprocess_texts_with_bert(test_data['TweetText'])\n","\n","# Convertir en TensorFlow Tensors\n","X_train_ids = tf.convert_to_tensor(X_train_ids, dtype=tf.int32)\n","X_train_mask = tf.convert_to_tensor(X_train_mask, dtype=tf.int32)\n","X_val_ids = tf.convert_to_tensor(X_val_ids, dtype=tf.int32)\n","X_val_mask = tf.convert_to_tensor(X_val_mask, dtype=tf.int32)\n","X_test_ids = tf.convert_to_tensor(X_test_ids, dtype=tf.int32)\n","X_test_mask = tf.convert_to_tensor(X_test_mask, dtype=tf.int32)\n","\n","y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n","y_val = tf.convert_to_tensor(y_val, dtype=tf.int32)\n","y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n","\n","def create_bert_model(max_len):\n","    # Define model inputs\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_len,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    attention_mask = tf.keras.layers.Input(\n","        shape=(max_len,), dtype=tf.int32, name=\"attention_mask\"\n","    )\n","\n","    # Define a custom layer that wraps TFBertModel\n","    class BertLayer(tf.keras.layers.Layer):\n","        def __init__(self, **kwargs):\n","            super(BertLayer, self).__init__(**kwargs)\n","            self.bert = TFBertModel.from_pretrained(\"bert-base-uncased\")\n","\n","        def call(self, inputs):\n","            input_ids, attention_mask = inputs\n","            outputs = self.bert(\n","                input_ids=input_ids, attention_mask=attention_mask\n","            )\n","            return outputs.last_hidden_state\n","\n","    # Use the custom BertLayer\n","    bert_outputs = BertLayer()([input_ids, attention_mask])\n","\n","    # Add CNN layers\n","    conv = Conv1D(128, kernel_size=3, activation=\"relu\")(bert_outputs)\n","    pool = GlobalMaxPooling1D()(conv)\n","    dense = Dense(64, activation=\"relu\")(pool)\n","    dropout = Dropout(0.5)(dense)\n","    output = Dense(1, activation=\"sigmoid\")(dropout)\n","\n","    # Build and compile the model\n","    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n","    model.compile(\n","        optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n","    )\n","    return model\n","\n","model = create_bert_model(max_len)\n","\n","# Entraîner et logguer le modèle avec MLflow\n","with mlflow.start_run(run_name='Optimized_BERT_CNN', nested=True):\n","    # Logguer les paramètres clés du modèle\n","    mlflow.log_param('model', 'BERT_CNN')\n","    mlflow.log_param('max_len', max_len)\n","\n","    # Sauvegarder le tokenizer comme artefact\n","    tokenizer.save_pretrained('bert_tokenizer')\n","    mlflow.log_artifacts('bert_tokenizer', artifact_path='tokenizer')\n","\n","    # Entraîner le modèle\n","    start_time = time.time()\n","    history = model.fit(\n","        [X_train_ids, X_train_mask],\n","        y_train,\n","        validation_data=([X_val_ids, X_val_mask], y_val),\n","        epochs=5,\n","        batch_size=32,\n","        verbose=1\n","    )\n","    end_time = time.time()\n","\n","    # Logguer le temps d'entraînement\n","    training_time = end_time - start_time\n","    mlflow.log_metric('training_time', training_time)\n","\n","    # Évaluer le modèle\n","    y_pred_prob = model.predict([X_test_ids, X_test_mask])\n","    y_pred = (y_pred_prob > 0.5).astype('int32')\n","    test_accuracy = accuracy_score(y_test, y_pred)\n","    test_f1 = f1_score(y_test, y_pred)\n","\n","    # Logguer les métriques finales\n","    mlflow.log_metric('test_accuracy', test_accuracy)\n","    mlflow.log_metric('test_f1_score', test_f1)\n","\n","    # Logguer le rapport de classification\n","    report = classification_report(y_test, y_pred)\n","    mlflow.log_text(report, 'classification_report_Optimized_BERT.txt')\n","\n","print(\"Training and evaluation complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":489},"id":"ZWJsiEYGboTz","outputId":"ba87edc8-dbb4-41bb-c0d7-d2cdd86049f1","executionInfo":{"status":"error","timestamp":1736098522920,"user_tz":-60,"elapsed":9325,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored in: <function _xla_gc_callback at 0x7d94209deb90>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 96, in _xla_gc_callback\n","    def _xla_gc_callback(*args):\n","KeyboardInterrupt: \n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'azureml'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ce44f8b69029>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azureml'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["!pip install mlflow --quiet"],"metadata":{"id":"lussTI-CPUra","executionInfo":{"status":"ok","timestamp":1736061697356,"user_tz":-60,"elapsed":3037,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["import os\n","import shutil\n","import pprint\n","\n","from random import random, randint\n","import mlflow.sklearn\n","from mlflow import log_metric, log_param, log_artifacts\n","from sklearn.ensemble import RandomForestRegressor\n","from mlflow.tracking import MlflowClient\n","import warnings"],"metadata":{"id":"8qV5YHdpWOEe","executionInfo":{"status":"ok","timestamp":1736061809168,"user_tz":-60,"elapsed":1070,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade mlflow\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"xA6US3D-AT_J","executionInfo":{"status":"ok","timestamp":1736156798505,"user_tz":-60,"elapsed":2933,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"e227cb38-1c38-426b-c110-e86ff71314c8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mlflow in /usr/local/lib/python3.10/dist-packages (2.19.0)\n","Requirement already satisfied: mlflow-skinny==2.19.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.19.0)\n","Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.0)\n","Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.1.4)\n","Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.14.0)\n","Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (7.1.0)\n","Requirement already satisfied: graphene<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.4.3)\n","Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.10/dist-packages (from mlflow) (23.0.0)\n","Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.7)\n","Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow) (3.8.0)\n","Requirement already satisfied: numpy<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.26.4)\n","Requirement already satisfied: pandas<3 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.2.2)\n","Requirement already satisfied: pyarrow<19,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (17.0.0)\n","Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.6.0)\n","Requirement already satisfied: scipy<2 in /usr/local/lib/python3.10/dist-packages (from mlflow) (1.13.1)\n","Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow) (2.0.36)\n","Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.1.7)\n","Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.0)\n","Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (0.40.0)\n","Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.43)\n","Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (8.5.0)\n","Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n","Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n","Requirement already satisfied: packaging<25 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (24.2)\n","Requirement already satisfied: protobuf<6,>=3.12.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (4.25.5)\n","Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.3)\n","Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.8)\n","Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic!=1.10.0,<2->mlflow) (4.12.2)\n","Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker<8,>=4.0.0->mlflow) (2.2.3)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (3.1.3)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (2.2.0)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow) (1.9.0)\n","Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.5)\n","Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (3.2.0)\n","Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from graphene<4->mlflow) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow) (3.2.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3->mlflow) (2024.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2->mlflow) (3.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\n","Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.10/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.27.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.11)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.21.0)\n","Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.2.15)\n","Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2024.12.14)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (1.17.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (5.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.1)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.6.1)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOiFFlcvAweS","executionInfo":{"status":"ok","timestamp":1736156848646,"user_tz":-60,"elapsed":3258,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"4f3956fc-6c7f-4f85-9950-3925e98e83a2"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["from mlflow.tracking import MlflowClient\n","\n","client = MlflowClient(tracking_uri=\"file:/content/drive/MyDrive/mlruns\")\n","experiments = client.search_experiments()\n","for e in experiments:\n","    print(f\"[ExpID={e.experiment_id}] name={e.name}, artifact={e.artifact_location}\")\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kEXOkU9QARPu","executionInfo":{"status":"ok","timestamp":1736156877318,"user_tz":-60,"elapsed":676,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"b5257c49-6eb6-427a-ed65-8143b7ed42ce"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["[ExpID=165120806529865697] name=Sentiment_Analysis_Models, artifact=file:///content/drive/MyDrive/mlruns/165120806529865697\n","[ExpID=0] name=Default, artifact=file:///content/drive/MyDrive/mlruns/0\n"]}]},{"cell_type":"code","source":["from mlflow.tracking import MlflowClient\n","\n","client = MlflowClient(tracking_uri=\"file:/content/drive/MyDrive/mlruns\")\n","\n","runs = client.search_runs(\n","    experiment_ids=[\"165120806529865697\"],\n","    order_by=[\"attributes.start_time DESC\"]\n",")\n","\n","for r in runs:\n","    print(r.info.run_id, r.data.metrics, r.data.params)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MjSwqh_fBAK3","executionInfo":{"status":"ok","timestamp":1736156961759,"user_tz":-60,"elapsed":39225,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"b508ee94-18ad-4592-d3a7-693df9b30dd0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["aa4eb14e57f9434eb742ce2a467785be {'training_time': 2467.786943435669, 'test_accuracy': 0.8079333333333333, 'test_f1_score': 0.7874271379030473} {'model': 'BERT_CNN', 'max_len': '50'}\n","56e1bbae513542dab8c277d8569fde8c {'mean_fit_time': 1.0809532006581624, 'std_fit_time': 0.06624618301233433, 'mean_score_time': 0.0229184627532959, 'std_score_time': 0.000701330475593082, 'mean_test_score': 0.7834468214116286, 'std_test_score': 0.0008506701444032002, 'rank_test_score': 1.0} {'C': '1', 'class_weight': 'None', 'dual': 'False', 'fit_intercept': 'True', 'intercept_scaling': '1', 'l1_ratio': 'None', 'max_iter': '1000', 'multi_class': 'deprecated', 'n_jobs': 'None', 'penalty': 'l2', 'random_state': '42', 'solver': 'liblinear', 'tol': '0.0001', 'verbose': '0', 'warm_start': 'False'}\n","7611f47ebac8451fb472a2800522c435 {'training_precision_score': 0.8002054931787631, 'training_recall_score': 0.8001071428571429, 'training_f1_score': 0.8000907697914157, 'training_accuracy_score': 0.8001071428571429, 'training_log_loss': 0.4402886361411917, 'training_roc_auc': 0.8802539084693878, 'training_score': 0.8018999483248034, 'best_cv_score': 0.7834468214116286, 'val_accuracy': 0.7805333333333333, 'val_f1_score': 0.783776683087028, 'test_accuracy': 0.7827, 'test_f1_score': 0.7844174741228215} {'model': 'Logistic Regression', 'vectorizer': 'TF-IDF', 'max_features': '5000', 'cv': '3', 'error_score': 'nan', 'estimator': 'LogisticRegression(random_state=42)', 'n_jobs': '-1', 'param_grid': \"{'C': [0.01, 0.1, 1, 10], 'solver': ['lbfgs', 'liblinear'], 'max_iter': [100, 500, 1000]}\", 'pre_dispatch': '2*n_jobs', 'refit': 'True', 'return_train_score': 'False', 'scoring': 'f1', 'verbose': '1', 'best_C': '1', 'best_max_iter': '100', 'best_solver': 'liblinear', 'C': '1', 'max_iter': '100', 'solver': 'liblinear'}\n","bad1b4347d634e8f884863d240d39305 {'mean_fit_time': 0.952178955078125, 'std_fit_time': 0.0181531948912171, 'mean_score_time': 0.022191762924194336, 'std_score_time': 0.00034815845317391365, 'mean_test_score': 0.7834468214116286, 'std_test_score': 0.0008506701444032002, 'rank_test_score': 1.0} {'C': '1', 'class_weight': 'None', 'dual': 'False', 'fit_intercept': 'True', 'intercept_scaling': '1', 'l1_ratio': 'None', 'max_iter': '100', 'multi_class': 'deprecated', 'n_jobs': 'None', 'penalty': 'l2', 'random_state': '42', 'solver': 'liblinear', 'tol': '0.0001', 'verbose': '0', 'warm_start': 'False'}\n","d0587f5a15a844f18922218f5139ddf7 {'mean_fit_time': 1.046241283416748, 'std_fit_time': 0.044980860224747087, 'mean_score_time': 0.022228082021077473, 'std_score_time': 0.0005193200460718553, 'mean_test_score': 0.7834468214116286, 'std_test_score': 0.0008506701444032002, 'rank_test_score': 1.0} {'C': '1', 'class_weight': 'None', 'dual': 'False', 'fit_intercept': 'True', 'intercept_scaling': '1', 'l1_ratio': 'None', 'max_iter': '500', 'multi_class': 'deprecated', 'n_jobs': 'None', 'penalty': 'l2', 'random_state': '42', 'solver': 'liblinear', 'tol': '0.0001', 'verbose': '0', 'warm_start': 'False'}\n","d4e4dcb878834985a768d94c3973c9e5 {'mean_fit_time': 0.3046737511952718, 'std_fit_time': 0.020579718068208675, 'mean_score_time': 0.021576484044392902, 'std_score_time': 0.0006902935645570286, 'mean_test_score': 0.7834427296463456, 'std_test_score': 0.0007988042005892378, 'rank_test_score': 4.0} {'C': '1', 'class_weight': 'None', 'dual': 'False', 'fit_intercept': 'True', 'intercept_scaling': '1', 'l1_ratio': 'None', 'max_iter': '100', 'multi_class': 'deprecated', 'n_jobs': 'None', 'penalty': 'l2', 'random_state': '42', 'solver': 'lbfgs', 'tol': '0.0001', 'verbose': '0', 'warm_start': 'False'}\n","f235801130ff4b6a8d9fe9f29477b254 {'mean_fit_time': 0.30218950907389325, 'std_fit_time': 0.028337939673841777, 'mean_score_time': 0.021255652109781902, 'std_score_time': 0.00027724061258952554, 'mean_test_score': 0.7834427296463456, 'std_test_score': 0.0007988042005892378, 'rank_test_score': 4.0} {'C': '1', 'class_weight': 'None', 'dual': 'False', 'fit_intercept': 'True', 'intercept_scaling': '1', 'l1_ratio': 'None', 'max_iter': '500', 'multi_class': 'deprecated', 'n_jobs': 'None', 'penalty': 'l2', 'random_state': '42', 'solver': 'lbfgs', 'tol': '0.0001', 'verbose': '0', 'warm_start': 'False'}\n","c46ec445eaf04953a4776c2ec1df3f63 {'training_time': 1867.60506772995, 'test_accuracy': 0.7855333333333333, 'test_f1_score': 0.7875165125495377} {'model': 'Light_Bidirectional_LSTM', 'embedding_dim': '50', 'max_features': '10000', 'max_len': '50'}\n","d923abfb594c415d963598cb37058f44 {'training_time': 2777.368332386017, 'test_accuracy': 0.7616, 'test_f1_score': 0.7592081341323816} {'model': 'Bidirectional_GRU_light', 'embedding': 'glove-twitter-50', 'embedding_dim': '50', 'max_features': '10000', 'max_len': '50'}\n","dcf0c5bb69c046ac99b301e0e2278063 {'training_time': 5351.820109367371, 'test_accuracy': 0.7627, 'test_f1_score': 0.7482762278561579} {'model': 'Bidirectional_GRU', 'embedding_dim': '100', 'max_features': '10000', 'max_len': '50'}\n","319e2540506a4a8fb9bd9a3abbd36a6d {'training_time': 261.2638809680939} {'model': 'BERT_CNN', 'max_len': '50'}\n"]}]},{"cell_type":"code","source":["!pip install mlflow pyngrok --quiet\n","\n","from pyngrok import ngrok\n","\n","# (Optionnel) tuer d’éventuels anciens tunnels\n","ngrok.kill()\n","\n","# Lancer MLflow en tâche de fond\n","get_ipython().system_raw('mlflow ui --backend-store-uri=\"file:/content/drive/MyDrive/mlruns\" --port 5000 &')\n","\n","# Ouvrir le tunnel ngrok\n","public_url = ngrok.connect(5000)\n","print(\"MLflow Tracking UI disponible à l'adresse :\", public_url.public_url)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kS1fphApBboK","executionInfo":{"status":"ok","timestamp":1736157072770,"user_tz":-60,"elapsed":2757,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"aeed9fd4-5af8-4c49-aefb-143d73d8e303"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["MLflow Tracking UI disponible à l'adresse : https://c3dd-34-138-144-142.ngrok-free.app\n"]}]},{"cell_type":"code","source":["import time\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import mlflow\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","from transformers import BertTokenizer, TFBertModel\n","from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout\n","from tensorflow.keras.models import Model\n","\n","\n","mlflow.set_tracking_uri(\"file:/content/drive/MyDrive/mlruns\")\n","mlflow.set_experiment(\"Sentiment_Analysis_Models\")\n","# --------------------------------------------------\n","# 1. Chargement des données\n","# --------------------------------------------------\n","data_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/train_data.csv'\n","balanced_data = pd.read_csv(data_path)\n","\n","# On garde uniquement les colonnes utiles\n","balanced_data = balanced_data[['target', 'TweetText']]\n","\n","# --------------------------------------------------\n","# 2. Division des données en train, validation, test\n","# --------------------------------------------------\n","train_data, temp_data = train_test_split(\n","    balanced_data, test_size=0.3, random_state=42, stratify=balanced_data['target']\n",")\n","val_data, test_data = train_test_split(\n","    temp_data, test_size=0.5, random_state=42, stratify=temp_data['target']\n",")\n","\n","# --------------------------------------------------\n","# 3. Préparer le tokenizer BERT\n","# --------------------------------------------------\n","max_len = 50\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def preprocess_texts_with_bert(texts):\n","    tokens = tokenizer(\n","        list(texts),\n","        max_length=max_len,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=\"tf\"\n","    )\n","    return tokens['input_ids'], tokens['attention_mask']\n","\n","# --------------------------------------------------\n","# 4. Prétraitement des données\n","# --------------------------------------------------\n","X_train_ids, X_train_mask = preprocess_texts_with_bert(train_data['TweetText'])\n","X_val_ids, X_val_mask = preprocess_texts_with_bert(val_data['TweetText'])\n","X_test_ids, X_test_mask = preprocess_texts_with_bert(test_data['TweetText'])\n","\n","# --------------------------------------------------\n","# 5. Conversion explicite en tf.Tensor\n","# --------------------------------------------------\n","X_train_ids = tf.convert_to_tensor(X_train_ids, dtype=tf.int32)\n","X_train_mask = tf.convert_to_tensor(X_train_mask, dtype=tf.int32)\n","X_val_ids = tf.convert_to_tensor(X_val_ids, dtype=tf.int32)\n","X_val_mask = tf.convert_to_tensor(X_val_mask, dtype=tf.int32)\n","X_test_ids = tf.convert_to_tensor(X_test_ids, dtype=tf.int32)\n","X_test_mask = tf.convert_to_tensor(X_test_mask, dtype=tf.int32)\n","\n","# Extraction des labels et conversion en tf.Tensor\n","y_train = tf.convert_to_tensor(train_data['target'].values, dtype=tf.int32)\n","y_val = tf.convert_to_tensor(val_data['target'].values, dtype=tf.int32)\n","y_test = tf.convert_to_tensor(test_data['target'].values, dtype=tf.int32)\n","\n","# --------------------------------------------------\n","# 6. Définition du modèle BERT + CNN\n","# --------------------------------------------------\n","def create_bert_model(max_len):\n","    \"\"\"\n","    Crée un modèle Keras en utilisant une couche personnalisée\n","    qui encapsule TFBertModel.\n","    \"\"\"\n","    # 1) Inputs du modèle\n","    input_ids = tf.keras.layers.Input(\n","        shape=(max_len,), dtype=tf.int32, name=\"input_ids\"\n","    )\n","    attention_mask = tf.keras.layers.Input(\n","        shape=(max_len,), dtype=tf.int32, name=\"attention_mask\"\n","    )\n","\n","    # 2) Définition de la couche BERT personnalisée\n","    class BertLayer(tf.keras.layers.Layer):\n","        def __init__(self, **kwargs):\n","            super(BertLayer, self).__init__(**kwargs)\n","            # On charge le modèle pré-entraîné de Hugging Face\n","            self.bert = TFBertModel.from_pretrained(\"bert-base-uncased\")\n","\n","        def call(self, inputs):\n","            input_ids, attention_mask = inputs\n","            outputs = self.bert(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask\n","            )\n","            # Renvoie uniquement la dernière couche cachée\n","            return outputs.last_hidden_state\n","\n","    # 3) Application de la couche BertLayer\n","    bert_outputs = BertLayer()([input_ids, attention_mask])\n","\n","    # 4) Ajout des couches CNN\n","    conv = Conv1D(128, kernel_size=3, activation=\"relu\")(bert_outputs)\n","    pool = GlobalMaxPooling1D()(conv)\n","    dense = Dense(64, activation=\"relu\")(pool)\n","    dropout = Dropout(0.5)(dense)\n","    output = Dense(1, activation=\"sigmoid\")(dropout)\n","\n","    # 5) Construction et compilation du modèle\n","    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n","    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n","\n","    return model\n","\n","# --------------------------------------------------\n","# 7. Instanciation du modèle\n","# --------------------------------------------------\n","model = create_bert_model(max_len)\n","\n","# --------------------------------------------------\n","# 8. Entraînement & suivi MLflow\n","# --------------------------------------------------\n","mlflow.set_experiment(\"Sentiment_Analysis_Models\")\n","with mlflow.start_run(run_name='Optimized_BERT_CNN', nested=True):\n","    # Log des hyperparamètres\n","    mlflow.log_param('model', 'BERT_CNN')\n","    mlflow.log_param('max_len', max_len)\n","\n","    # Sauvegarde du tokenizer\n","    tokenizer.save_pretrained('bert_tokenizer')\n","    mlflow.log_artifacts('bert_tokenizer', artifact_path='tokenizer')\n","\n","    # Entraînement\n","    start_time = time.time()\n","    history = model.fit(\n","        [X_train_ids, X_train_mask],\n","        y_train,\n","        validation_data=([X_val_ids, X_val_mask], y_val),\n","        epochs=5,\n","        batch_size=32,\n","        verbose=1\n","    )\n","    end_time = time.time()\n","\n","    # Log du temps d'entraînement\n","    training_time = end_time - start_time\n","    mlflow.log_metric('training_time', training_time)\n","\n","    # Évaluation\n","    y_pred_prob = model.predict([X_test_ids, X_test_mask])  # Numpy array\n","    y_pred = (y_pred_prob > 0.5).astype('int32')            # Numpy array (CPU)\n","\n","# Convertir y_test en Numpy array (CPU)\n","    y_test_cpu = y_test.numpy()\n","\n","    test_accuracy = accuracy_score(y_test_cpu, y_pred)\n","    test_f1 = f1_score(y_test_cpu, y_pred)\n","\n","    mlflow.log_metric('test_accuracy', test_accuracy)\n","    mlflow.log_metric('test_f1_score', test_f1)\n","\n","# Rapport de classification\n","    report = classification_report(y_test_cpu, y_pred)\n","    print(\"Classification Report:\\n\", report)\n","\n","print(\"Training and evaluation complete.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":796,"referenced_widgets":["7120f27c0eb54812b36a3fa60a253df6","bfbe14ad18484e8286a212f1d92473ac","dcbc8641589e4eb6bd33ef9bdbd26266","7b07b6a83a0b4ea497c425ae9e591e91","cae813044bd248879dbec98bd1aea171","b3efc077310b4f38b4dc42c3345a3ed5","a2f9c186af6e405293fcd79c07f1f572","38e48e83164146c2aa0e3dfa8da870d8","8effdeefab004d5eb3f35ba4eb657e10","dd92196a116f48a1acc05b17d9f83021","050663d5ed8e4318ac7f063e6d495c42","60ef8d7b5dc5409db0e9a85d9695bbab","c04e0efcdd8b425bbb614954991cdef9","df74402a94574e0793836112032fd99c","835cfc25db4f43a79db754394768bf40","af6a8d68e68640c6bfe416cb45afae6c","3c4b865c26024574adaf8959f554678f","51e9ae2eeafe4ef6bc53099bfcab7cb0","20220f90a63c4f18915540f2783e1947","f9198722b40f44dcbd4523cbce2d8e72","680a7a3ff165439a9730c88188fe5a18","09cc98fa496741758b774d31f1ff775d","3f334947127d44c8b3ee44918d2ca933","01e3aa6e07ef4d0f9b46ce1931320def","e78dedec9c814ab293b804c1e81c4a16","ad810c946d234b0bb30c9bdff208fb44","8a9bfddef3434a6c8b28201eb5326063","d7dc2e691ddd433d9f37cb7cb356bf8e","ad573b1732cb440ba3d077af39af6334","b06417e3cb0e429d8355142612dfaf12","2d19240c37a241d6a29f27b3f1d51226","ca5d6bc94f094f45bfa3c823a5d66b34","fa4f98b5d615432bb8577fa98eb4b9c6","c654b00ac601409985492c64d4d4b839","2519b66dd98f43daa8749c77fd6dc06e","a16595e2883a445589a1410a659865af","aee64cd2a2ca4cb0887f02df370a4f08","3e5437c01143454aa6d751868f32324b","1429522ede914476b382d50721357058","3ed5522d85964531a879c082012866a9","5ee6fb265f2f4ae3bb06a49ff0484819","b6888be7e9f044619c80ec1eed8d90c0","73bd37d0fc2a4705b478fd48aaef24bd","267004797b4d49f6becc12f43cc6b246","a31cce936ed44a0e94cf732cea699419","f4a992e8b06c4b2599004d14c48e7127","64dcf30234fb4e1c98409dabad1b9c57","2d1eabc095af4b33a64da87b590d4a13","768ec08a908540fa9d3c4f7b60e00c85","16f5f37b6bd64d5ba6f23290bbe400d1","84231cf2b0db4f00a71f5d24878b9a6d","fbb9835c70df45bfb2d6b4b8c5a07afa","4ea4e3c3dfe645e98e3ba3824673ac95","bc49257937da4382abf261e081387234","acd89078cc364594b4fa28bc476a0665"]},"id":"hM-dxujTM6l1","executionInfo":{"status":"ok","timestamp":1736101181080,"user_tz":-60,"elapsed":2653311,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"aed2fd97-ecae-4664-bf67-59709e47dc3a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7120f27c0eb54812b36a3fa60a253df6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60ef8d7b5dc5409db0e9a85d9695bbab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f334947127d44c8b3ee44918d2ca933"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c654b00ac601409985492c64d4d4b839"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a31cce936ed44a0e94cf732cea699419"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","\u001b[1m4375/4375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 114ms/step - accuracy: 0.7763 - loss: 0.4786 - val_accuracy: 0.8149 - val_loss: 0.4053\n","Epoch 2/5\n","\u001b[1m4375/4375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 112ms/step - accuracy: 0.8226 - loss: 0.4026 - val_accuracy: 0.8209 - val_loss: 0.3918\n","Epoch 3/5\n","\u001b[1m4375/4375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m489s\u001b[0m 112ms/step - accuracy: 0.8356 - loss: 0.3760 - val_accuracy: 0.8244 - val_loss: 0.3867\n","Epoch 4/5\n","\u001b[1m4375/4375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 112ms/step - accuracy: 0.8447 - loss: 0.3527 - val_accuracy: 0.8242 - val_loss: 0.4003\n","Epoch 5/5\n","\u001b[1m4375/4375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m488s\u001b[0m 112ms/step - accuracy: 0.8547 - loss: 0.3314 - val_accuracy: 0.8067 - val_loss: 0.4208\n","\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 96ms/step\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.76      0.90      0.82     15000\n","           1       0.88      0.71      0.79     15000\n","\n","    accuracy                           0.81     30000\n","   macro avg       0.82      0.81      0.81     30000\n","weighted avg       0.82      0.81      0.81     30000\n","\n","Training and evaluation complete.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9C8IxwYEM7lt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Charger le fichier test_data.csv\n","test_data_path = '/content/drive/MyDrive/Réalisez une analyse de sentiments grâce au Deep Learning/test_data.csv'\n","test_data = pd.read_csv(test_data_path)\n","\n","# Afficher les premières lignes pour vérifier\n","display(test_data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"a99OExc_BTnE","executionInfo":{"status":"ok","timestamp":1732903507756,"user_tz":-60,"elapsed":225,"user":{"displayName":"Julien CANTALAPIEDRA","userId":"12978492932510530161"}},"outputId":"2fa2539a-a376-4f4b-c01d-6cb3ae30447d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["   target         ids                          date      flag           user  \\\n","0       0  2255153540  Sat Jun 20 10:50:18 PDT 2009  NO_QUERY  Charmed231281   \n","1       1  1881316460  Fri May 22 04:18:01 PDT 2009  NO_QUERY      natneagle   \n","2       1  2056281230  Sat Jun 06 10:51:32 PDT 2009  NO_QUERY        HippyDi   \n","3       0  1556845050  Sun Apr 19 00:37:54 PDT 2009  NO_QUERY      stephyang   \n","4       0  1833915956  Mon May 18 00:55:46 PDT 2009  NO_QUERY   anbudan_BALA   \n","\n","                                           TweetText  \n","0        Today i have night work at the airport fun   \n","1                  @ra1ne yea  so glad you liked it!  \n","2        @orangy68 Thats really sweet...back at you   \n","3  Holi show was fun.. after party at ten pin was...  \n","4  @peyarili I may not agree with certain things ...  "],"text/html":["\n","  <div id=\"df-1e8cb634-2fa7-4be6-b76d-ca2fc0125a30\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>ids</th>\n","      <th>date</th>\n","      <th>flag</th>\n","      <th>user</th>\n","      <th>TweetText</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>2255153540</td>\n","      <td>Sat Jun 20 10:50:18 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Charmed231281</td>\n","      <td>Today i have night work at the airport fun</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1881316460</td>\n","      <td>Fri May 22 04:18:01 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>natneagle</td>\n","      <td>@ra1ne yea  so glad you liked it!</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>2056281230</td>\n","      <td>Sat Jun 06 10:51:32 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>HippyDi</td>\n","      <td>@orangy68 Thats really sweet...back at you</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1556845050</td>\n","      <td>Sun Apr 19 00:37:54 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>stephyang</td>\n","      <td>Holi show was fun.. after party at ten pin was...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1833915956</td>\n","      <td>Mon May 18 00:55:46 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>anbudan_BALA</td>\n","      <td>@peyarili I may not agree with certain things ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e8cb634-2fa7-4be6-b76d-ca2fc0125a30')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-1e8cb634-2fa7-4be6-b76d-ca2fc0125a30 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-1e8cb634-2fa7-4be6-b76d-ca2fc0125a30');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-6db65a03-01b9-4faa-a918-b84b223b5d4e\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6db65a03-01b9-4faa-a918-b84b223b5d4e')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-6db65a03-01b9-4faa-a918-b84b223b5d4e button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(test_data\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 260592832,\n        \"min\": 1556845050,\n        \"max\": 2255153540,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1881316460,\n          1833915956\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Fri May 22 04:18:01 PDT 2009\",\n          \"Mon May 18 00:55:46 PDT 2009\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"flag\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"NO_QUERY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"natneagle\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"TweetText\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"@ra1ne yea  so glad you liked it!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]}]}